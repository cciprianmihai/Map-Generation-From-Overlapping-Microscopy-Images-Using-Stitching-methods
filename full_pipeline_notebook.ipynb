{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "olH4QCn7eEwD"
   },
   "outputs": [],
   "source": [
    "# INPUT_FOLDER = \"folder/to/big/images\"\n",
    "# OUTPUT_FOLDER = \"folder/where/results/are/stored\"\n",
    "INPUT_FOLDER = \"Dataset_saved/images\"\n",
    "OUTPUT_FOLDER = \"Dataset_saved/images_result_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sHwTGjAyD-Gx",
    "outputId": "f01643da-02fa-410a-91ea-d61d8627881d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LightGlue' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/cvg/LightGlue.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76UtLt4SD-p3",
    "outputId": "8cd47c58-bf8a-4cc0-8642-970f9ae6dfd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/apollo/Desktop/ImageStitching_Sebi_Buta/LightGlue\n"
     ]
    }
   ],
   "source": [
    "cd LightGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdZIS0h-D_iN",
    "outputId": "560da184-ce2b-481c-eeaa-b9fc44e1d766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/apollo/Desktop/ImageStitching_Sebi_Buta/LightGlue\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.9.1 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from lightglue==0.0) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision>=0.3 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from lightglue==0.0) (0.16.0+cu118)\n",
      "Requirement already satisfied: numpy in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from lightglue==0.0) (1.26.0)\n",
      "Requirement already satisfied: opencv-python in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from lightglue==0.0) (4.11.0.86)\n",
      "Requirement already satisfied: matplotlib in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from lightglue==0.0) (3.8.0)\n",
      "Requirement already satisfied: kornia>=0.6.11 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from lightglue==0.0) (0.8.1)\n",
      "Requirement already satisfied: kornia_rs>=0.1.9 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from kornia>=0.6.11->lightglue==0.0) (0.1.9)\n",
      "Requirement already satisfied: packaging in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from kornia>=0.6.11->lightglue==0.0) (23.1)\n",
      "Requirement already satisfied: filelock in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from torch>=1.9.1->lightglue==0.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from torch>=1.9.1->lightglue==0.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from torch>=1.9.1->lightglue==0.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from torch>=1.9.1->lightglue==0.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from torch>=1.9.1->lightglue==0.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from torch>=1.9.1->lightglue==0.0) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from torch>=1.9.1->lightglue==0.0) (2.1.0)\n",
      "Requirement already satisfied: requests in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from torchvision>=0.3->lightglue==0.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from torchvision>=0.3->lightglue==0.0) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from matplotlib->lightglue==0.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from matplotlib->lightglue==0.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from matplotlib->lightglue==0.0) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from matplotlib->lightglue==0.0) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from matplotlib->lightglue==0.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from matplotlib->lightglue==0.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->lightglue==0.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from jinja2->torch>=1.9.1->lightglue==0.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from requests->torchvision>=0.3->lightglue==0.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from requests->torchvision>=0.3->lightglue==0.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from requests->torchvision>=0.3->lightglue==0.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from requests->torchvision>=0.3->lightglue==0.0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/apollo/anaconda3/envs/medvit/lib/python3.11/site-packages (from sympy->torch>=1.9.1->lightglue==0.0) (1.3.0)\n",
      "Building wheels for collected packages: lightglue\n",
      "  Building editable for lightglue (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lightglue: filename=lightglue-0.0-0.editable-py3-none-any.whl size=14975 sha256=a629ed0fada245624633a6c87de15daf3ba72cd2ee9fae82b45acdf73fb8c351\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d4ptv8b3/wheels/7d/72/ce/3a84345f21b2056f4113eb11d2af5660aaeb948fc32c887067\n",
      "Successfully built lightglue\n",
      "Installing collected packages: lightglue\n",
      "  Attempting uninstall: lightglue\n",
      "    Found existing installation: lightglue 0.0\n",
      "    Uninstalling lightglue-0.0:\n",
      "      Successfully uninstalled lightglue-0.0\n",
      "Successfully installed lightglue-0.0\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TjkJmKx8eAIE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import random\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import json\n",
    "from typing import List\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lightglue import LightGlue, SuperPoint, SIFT\n",
    "from lightglue.utils import load_image, rbd\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
    "plt.rcParams['figure.figsize'] = [15, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIOYROTTHvCH",
    "outputId": "b0dc1532-68e6-49d4-c16c-56b5a902d602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/apollo/Desktop/ImageStitching_Sebi_Buta\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GQdYtXuJeEyF"
   },
   "outputs": [],
   "source": [
    "name_map = {\n",
    "    \"0000.png\": \"0000.png\",\n",
    "    \"0005.png\": \"0001.png\",\n",
    "    \"0001.png\": \"0002.png\",\n",
    "    \"0010.png\": \"0003.png\",\n",
    "    \"0006.png\": \"0004.png\",\n",
    "    \"0002.png\": \"0005.png\",\n",
    "    \"0015.png\": \"0006.png\",\n",
    "    \"0011.png\": \"0007.png\",\n",
    "    \"0007.png\": \"0008.png\",\n",
    "    \"0003.png\": \"0009.png\",\n",
    "    \"0020.png\": \"0010.png\",\n",
    "    \"0016.png\": \"0011.png\",\n",
    "    \"0012.png\": \"0012.png\",\n",
    "    \"0008.png\": \"0013.png\",\n",
    "    \"0004.png\": \"0014.png\",\n",
    "    \"0021.png\": \"0015.png\",\n",
    "    \"0017.png\": \"0016.png\",\n",
    "    \"0013.png\": \"0017.png\",\n",
    "    \"0009.png\": \"0018.png\",\n",
    "    \"0022.png\": \"0019.png\",\n",
    "    \"0018.png\": \"0020.png\",\n",
    "    \"0014.png\": \"0021.png\",\n",
    "    \"0023.png\": \"0022.png\",\n",
    "    \"0019.png\": \"0023.png\",\n",
    "    \"0024.png\": \"0024.png\"\n",
    "}\n",
    "\n",
    "def rename_and_copy_images(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            base_name = os.path.basename(filename)\n",
    "            if base_name in name_map:\n",
    "                new_name = name_map[base_name]\n",
    "                src = os.path.join(input_folder, filename)\n",
    "                dst = os.path.join(output_folder, new_name)\n",
    "                shutil.copyfile(src, dst)\n",
    "                # print(f\"Copied {filename} as {new_name}\")\n",
    "            else:\n",
    "                print(f\"Skipping {filename}: not in mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7-1AZ8w6eE0c"
   },
   "outputs": [],
   "source": [
    "def pad_images(\n",
    "    input_folder: str,\n",
    "    output_folder: str,\n",
    "    padding_fraction: float = 0.2\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pads all images in the input folder with black borders and saves them to the output folder.\n",
    "\n",
    "    Parameters:\n",
    "    - input_folder (str): Path to the folder containing input images.\n",
    "    - output_folder (str): Path where padded images will be saved.\n",
    "    - padding_fraction (float): The fraction of 1/5 of the image's height and width to be used as padding.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Supported image extensions\n",
    "    valid_extensions = ('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.svs')\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(valid_extensions):\n",
    "            img_path = os.path.join(input_folder, filename)\n",
    "            img: Optional[np.ndarray] = cv2.imread(img_path)\n",
    "\n",
    "            if img is None:\n",
    "                print(f\"Could not read image: {filename}\")\n",
    "                continue\n",
    "\n",
    "            h, w = img.shape[:2]\n",
    "\n",
    "            # Calculate padding amounts (20% of 1/5 of original size)\n",
    "            pad_height_total = int(h * padding_fraction * 0.2)\n",
    "            pad_width_total = int(w * padding_fraction * 0.2)\n",
    "\n",
    "            # Split total padding equally on both sides\n",
    "            pad_top = pad_bottom = pad_height_total // 2\n",
    "            pad_left = pad_right = pad_width_total // 2\n",
    "\n",
    "            # Apply black border padding\n",
    "            padded_img = cv2.copyMakeBorder(\n",
    "                img,\n",
    "                top=pad_top,\n",
    "                bottom=pad_bottom,\n",
    "                left=pad_left,\n",
    "                right=pad_right,\n",
    "                borderType=cv2.BORDER_CONSTANT,\n",
    "                value=[0, 0, 0]  # Black color\n",
    "            )\n",
    "\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(output_path, padded_img)\n",
    "            print(f\"Padded and saved: {filename} → {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fVKRYE0DeE2f"
   },
   "outputs": [],
   "source": [
    "def split_image_into_patches(image_path, output_dir, grid_size=(5, 5)):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    img = Image.open(image_path)\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    # Overlap ranges (as before)\n",
    "    h_overlap_min, h_overlap_max = 0.17, 0.23  # 20% ± 3%\n",
    "    v_overlap_min, v_overlap_max = 0.175, 0.225  # 20% ± 2.5%\n",
    "\n",
    "    # Random overlaps per row/column\n",
    "    h_overlaps = [random.uniform(h_overlap_min, h_overlap_max) for _ in range(grid_size[0] - 1)]\n",
    "    v_overlaps = [random.uniform(v_overlap_min, v_overlap_max) for _ in range(grid_size[1] - 1)]\n",
    "\n",
    "    # Calculate patch dimensions (approximate)\n",
    "    avg_h_overlap = sum(h_overlaps) / len(h_overlaps)\n",
    "    avg_v_overlap = sum(v_overlaps) / len(v_overlaps)\n",
    "    patch_width = img_width / (grid_size[0] - (grid_size[0] - 1) * avg_h_overlap)\n",
    "    patch_height = img_height / (grid_size[1] - (grid_size[1] - 1) * avg_v_overlap)\n",
    "\n",
    "    # Random translational offsets (±5% of patch size)\n",
    "    max_offset_x = int(patch_width * 0.05)\n",
    "    max_offset_y = int(patch_height * 0.05)\n",
    "\n",
    "    patch_num = 0\n",
    "    for row in range(grid_size[1]):\n",
    "        for col in range(grid_size[0]):\n",
    "            # Base position (with overlaps)\n",
    "            h_step = patch_width * (1 - (h_overlaps[col - 1] if col > 0 else 0))\n",
    "            v_step = patch_height * (1 - (v_overlaps[row - 1] if row > 0 else 0))\n",
    "\n",
    "            left = int(col * h_step)\n",
    "            upper = int(row * v_step)\n",
    "            right = int(left + patch_width)\n",
    "            lower = int(upper + patch_height)\n",
    "\n",
    "            # Apply random offsets (misalignment)\n",
    "            offset_x = random.randint(-max_offset_x, max_offset_x)\n",
    "            offset_y = random.randint(-max_offset_y, max_offset_y)\n",
    "\n",
    "            left += offset_x\n",
    "            upper += offset_y\n",
    "            right += offset_x\n",
    "            lower += offset_y\n",
    "\n",
    "            # Clamp to image bounds\n",
    "            left = max(0, left)\n",
    "            upper = max(0, upper)\n",
    "            right = min(img_width, right)\n",
    "            lower = min(img_height, lower)\n",
    "\n",
    "            # Crop and save\n",
    "            patch = img.crop((left, upper, right, lower))\n",
    "            patch_name = f\"{patch_num:04d}.png\"\n",
    "            patch.save(os.path.join(output_dir, patch_name))\n",
    "            patch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Lgphbnw-mhwW"
   },
   "outputs": [],
   "source": [
    "# Read image and convert them to gray!!\n",
    "def read_image(path, resize=1.0):\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found at path: {path}\")\n",
    "\n",
    "    # Resize to 80% of original size\n",
    "    height, width = img.shape[:2]\n",
    "    new_size = (int(width * 0.8), int(height * 0.8))\n",
    "    img = cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return img_gray, img, img_rgb\n",
    "\n",
    "def KAZE(img):\n",
    "    kazeDetector = cv2.KAZE_create()  # Create KAZE detector (supports both AKAZE and KAZE)\n",
    "\n",
    "    kp, des = kazeDetector.detectAndCompute(img, None)\n",
    "    return kp, des\n",
    "\n",
    "def plot_kaze(gray, rgb, kp):\n",
    "    tmp = rgb.copy()\n",
    "    img = cv2.drawKeypoints(gray, kp, tmp, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    return img\n",
    "\n",
    "def matcher(kp1, des1, img1, kp2, des2, img2, threshold):\n",
    "    # BFMatcher with default params\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1,des2, k=2)\n",
    "\n",
    "    # Apply ratio test\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < threshold*n.distance:\n",
    "            good.append([m])\n",
    "\n",
    "    matches = []\n",
    "    for pair in good:\n",
    "        matches.append(list(kp1[pair[0].queryIdx].pt + kp2[pair[0].trainIdx].pt))\n",
    "\n",
    "    matches = np.array(matches)\n",
    "    return matches\n",
    "\n",
    "def homography(pairs):\n",
    "    rows = []\n",
    "    for i in range(pairs.shape[0]):\n",
    "        p1 = np.append(pairs[i][0:2], 1)\n",
    "        p2 = np.append(pairs[i][2:4], 1)\n",
    "        row1 = [0, 0, 0, p1[0], p1[1], p1[2], -p2[1]*p1[0], -p2[1]*p1[1], -p2[1]*p1[2]]\n",
    "        row2 = [p1[0], p1[1], p1[2], 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1], -p2[0]*p1[2]]\n",
    "        rows.append(row1)\n",
    "        rows.append(row2)\n",
    "    rows = np.array(rows)\n",
    "    U, s, V = np.linalg.svd(rows)\n",
    "    H = V[-1].reshape(3, 3)\n",
    "    H = H/H[2, 2] # standardize to let w*H[2,2] = 1\n",
    "    return H\n",
    "\n",
    "def random_point(matches, k=4):\n",
    "    idx = random.sample(range(len(matches)), k)\n",
    "    point = [matches[i] for i in idx ]\n",
    "    return np.array(point)\n",
    "\n",
    "def get_error(points, H):\n",
    "    num_points = len(points)\n",
    "    all_p1 = np.concatenate((points[:, 0:2], np.ones((num_points, 1))), axis=1)\n",
    "    all_p2 = points[:, 2:4]\n",
    "    estimate_p2 = np.zeros((num_points, 2))\n",
    "    for i in range(num_points):\n",
    "        temp = np.dot(H, all_p1[i])\n",
    "        estimate_p2[i] = (temp/temp[2])[0:2] # set index 2 to 1 and slice the index 0, 1\n",
    "    # Compute error\n",
    "    errors = np.linalg.norm(all_p2 - estimate_p2 , axis=1) ** 2\n",
    "\n",
    "    return errors\n",
    "\n",
    "def ransac(matches, threshold, iters):\n",
    "    num_best_inliers = 0\n",
    "\n",
    "    for i in range(iters):\n",
    "        points = random_point(matches)\n",
    "        H = homography(points)\n",
    "\n",
    "        #  avoid dividing by zero\n",
    "        if np.linalg.matrix_rank(H) < 3:\n",
    "            continue\n",
    "\n",
    "        errors = get_error(matches, H)\n",
    "        idx = np.where(errors < threshold)[0]\n",
    "        inliers = matches[idx]\n",
    "\n",
    "        num_inliers = len(inliers)\n",
    "        if num_inliers > num_best_inliers:\n",
    "            best_inliers = inliers.copy()\n",
    "            num_best_inliers = num_inliers\n",
    "            best_H = H.copy()\n",
    "\n",
    "    # print(\"inliers/matches: {}/{}\".format(num_best_inliers, len(matches)))\n",
    "    return best_inliers, best_H\n",
    "\n",
    "def stitch_img(left, right, H):\n",
    "    # print(\"Stitching image ...\")\n",
    "\n",
    "    # Normalize images to float in [0,1]\n",
    "    left = cv2.normalize(left.astype('float'), None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "    right = cv2.normalize(right.astype('float'), None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Get dimensions of both images\n",
    "    height_l, width_l, _ = left.shape\n",
    "    height_r, width_r, _ = right.shape\n",
    "\n",
    "    # Compute corners for the left image and transform them using H\n",
    "    corners_left = np.array([[0, 0, 1],\n",
    "                             [width_l, 0, 1],\n",
    "                             [width_l, height_l, 1],\n",
    "                             [0, height_l, 1]]).T  # shape (3,4)\n",
    "    warped_corners_left = H @ corners_left\n",
    "    warped_corners_left /= warped_corners_left[2, :]  # Normalize homogeneous coordinates\n",
    "\n",
    "    # Compute corners for the right image (identity transform)\n",
    "    corners_right = np.array([[0, 0, 1],\n",
    "                              [width_r, 0, 1],\n",
    "                              [width_r, height_r, 1],\n",
    "                              [0, height_r, 1]]).T  # shape (3,4)\n",
    "\n",
    "    # Combine corners to find overall bounds\n",
    "    all_x = np.concatenate((warped_corners_left[0, :], corners_right[0, :]))\n",
    "    all_y = np.concatenate((warped_corners_left[1, :], corners_right[1, :]))\n",
    "\n",
    "    min_x, max_x = np.min(all_x), np.max(all_x)\n",
    "    min_y, max_y = np.min(all_y), np.max(all_y)\n",
    "\n",
    "    # Create a translation matrix to shift all images so that no coordinate is negative\n",
    "    tx = -min_x if min_x < 0 else 0\n",
    "    ty = -min_y if min_y < 0 else 0\n",
    "    translation_mat = np.array([[1, 0, tx],\n",
    "                                [0, 1, ty],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "    # New canvas size: use ceiling to ensure full coverage\n",
    "    width_new = int(np.ceil(max_x - min_x))\n",
    "    height_new = int(np.ceil(max_y - min_y))\n",
    "    size = (width_new, height_new)\n",
    "\n",
    "    # Warp left image with the composite transform: translation_mat @ H\n",
    "    warped_left = cv2.warpPerspective(left, translation_mat @ H, size)\n",
    "    # Warp right image with just the translation matrix (identity warp + translation)\n",
    "    warped_right = cv2.warpPerspective(right, translation_mat, size)\n",
    "\n",
    "    # Vectorized blending:\n",
    "    # Create masks where any channel is non-zero (assumed as non-black)\n",
    "    mask_left = np.any(warped_left != 0, axis=2)\n",
    "    mask_right = np.any(warped_right != 0, axis=2)\n",
    "\n",
    "    # Initialize the stitched image as black\n",
    "    stitch_image = np.zeros_like(warped_left)\n",
    "\n",
    "    # Pixels only from left image\n",
    "    only_left = mask_left & ~mask_right\n",
    "    stitch_image[only_left] = warped_left[only_left]\n",
    "\n",
    "    # Pixels only from right image\n",
    "    only_right = mask_right & ~mask_left\n",
    "    stitch_image[only_right] = warped_right[only_right]\n",
    "\n",
    "    # Pixels where both images contribute (average the pixel values)\n",
    "    both = mask_left & mask_right\n",
    "    stitch_image[both] = (warped_left[both] + warped_right[both]) / 2\n",
    "\n",
    "    return stitch_image\n",
    "def mask_image_top_left(img_gray, k):\n",
    "    \"\"\"\n",
    "    Keeps the top k% rows and left-most k% columns intact, rest set to black.\n",
    "\n",
    "    Parameters:\n",
    "        img_gray (np.ndarray): Grayscale image.\n",
    "        k (float): Percentage (0 < k <= 100) of image to retain in top and left.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Modified image.\n",
    "    \"\"\"\n",
    "    if not (0 < k <= 100):\n",
    "        raise ValueError(\"k must be between 0 and 100 (exclusive of 0).\")\n",
    "\n",
    "    h, w = img_gray.shape\n",
    "    mask = np.zeros_like(img_gray)\n",
    "\n",
    "    # Calculate cutoff indices\n",
    "    cutoff_row = int(h * k / 100)\n",
    "    cutoff_col = int(w * k / 100)\n",
    "\n",
    "    # Retain top k% rows\n",
    "    mask[:cutoff_row, :] = img_gray[:cutoff_row, :]\n",
    "\n",
    "    # Retain left-most k% columns\n",
    "    mask[:, :cutoff_col] = img_gray[:, :cutoff_col]\n",
    "\n",
    "    return mask\n",
    "\n",
    "def detect_black_border_depths(image_pil, black_threshold=10):\n",
    "    \"\"\"\n",
    "    Detects how much black padding exists on each edge of the image separately.\n",
    "\n",
    "    Args:\n",
    "        image_pil: PIL.Image object (RGB or grayscale).\n",
    "        black_threshold: Maximum pixel value considered \"black\" (0-255).\n",
    "\n",
    "    Returns:\n",
    "        (top, bottom, left, right): number of black pixels to crop from each side.\n",
    "    \"\"\"\n",
    "    image_np = np.array(image_pil)\n",
    "\n",
    "    if image_np.ndim == 3:\n",
    "        # Convert to grayscale if RGB\n",
    "        image_np = np.mean(image_np, axis=2)\n",
    "\n",
    "    h, w = image_np.shape\n",
    "\n",
    "    # Initialize cropping values\n",
    "    top = 0\n",
    "    bottom = 0\n",
    "    left = 0\n",
    "    right = 0\n",
    "\n",
    "    # Detect top\n",
    "    for y in range(h):\n",
    "        if np.all(image_np[y, :] <= black_threshold):\n",
    "            top += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Detect bottom\n",
    "    for y in range(h-1, -1, -1):\n",
    "        if np.all(image_np[y, :] <= black_threshold):\n",
    "            bottom += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Detect left\n",
    "    for x in range(w):\n",
    "        if np.all(image_np[:, x] <= black_threshold):\n",
    "            left += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Detect right\n",
    "    for x in range(w-1, -1, -1):\n",
    "        if np.all(image_np[:, x] <= black_threshold):\n",
    "            right += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return top, bottom, left, right\n",
    "\n",
    "def crop_black_borders(image_pil, black_threshold=10):\n",
    "    \"\"\"\n",
    "    Crop black borders independently from each edge.\n",
    "\n",
    "    Args:\n",
    "        image_pil: PIL.Image object (RGB or grayscale).\n",
    "        black_threshold: Maximum pixel value considered \"black\" (0-255).\n",
    "\n",
    "    Returns:\n",
    "        Cropped PIL.Image object.\n",
    "    \"\"\"\n",
    "    top, bottom, left, right = detect_black_border_depths(image_pil, black_threshold)\n",
    "    w, h = image_pil.size\n",
    "\n",
    "    # Calculate new box\n",
    "    left_crop = left\n",
    "    upper_crop = top\n",
    "    right_crop = w - right\n",
    "    lower_crop = h - bottom\n",
    "\n",
    "    if left_crop >= right_crop or upper_crop >= lower_crop:\n",
    "        # If everything is cropped away, return original (or could raise an error)\n",
    "        print(\"Warning: Cropping would remove entire image. Returning original.\")\n",
    "        return image_pil\n",
    "\n",
    "    return image_pil.crop((left_crop, upper_crop, right_crop, lower_crop))\n",
    "\n",
    "def KAZE_matching(img_path1, img_path2, output_dir=\"./kaze_output\", kaze_thresholding=0.7, ransac_thresholding=100, percentage_of_image_used=1.0):\n",
    "  left_gray, left_origin, left_rgb = read_image(img_path1)\n",
    "  right_gray, right_origin, right_rgb = read_image(img_path2)\n",
    "\n",
    "  right_gray = mask_image_top_left(right_gray, percentage_of_image_used * 100)\n",
    "\n",
    "  kp_left, des_left = KAZE(left_gray)\n",
    "  kp_right, des_right = KAZE(right_gray)\n",
    "\n",
    "  matches = matcher(kp_left, des_left, left_rgb, kp_right, des_right, right_rgb, 0.7)\n",
    "\n",
    "  inliers, H = ransac(matches, 100, 2000)\n",
    "\n",
    "  kaze_stitched = stitch_img(left_rgb, right_rgb, H)\n",
    "\n",
    "  kaze_stitched_uint8 = (kaze_stitched * 255).astype(np.uint8)\n",
    "  image_pil = Image.fromarray(kaze_stitched_uint8)\n",
    "\n",
    "  output_filename = f\"0000-{os.path.splitext(os.path.basename(img_path2))[0]}_KAZE_{kaze_thresholding}_{percentage_of_image_used}.jpg\"\n",
    "  output_filepath = os.path.join(output_dir, output_filename)\n",
    "\n",
    "  cropped_image = crop_black_borders(image_pil, black_threshold=10)\n",
    "  cropped_image.save(output_filepath, \"JPEG\", quality=95)\n",
    "\n",
    "  return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NBU5opE-mhyp"
   },
   "outputs": [],
   "source": [
    "def torch_to_cv2_image(tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a C×H×W RGB torch tensor (on CPU or CUDA) to a H×W×C BGR uint8 numpy array.\n",
    "    \"\"\"\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()\n",
    "    image = tensor.clone().detach()\n",
    "    # (C,H,W) -> (H,W,C)\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    # assume floats in [0,1] or ints in [0,255]\n",
    "    if image.dtype != np.uint8:\n",
    "        image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    # RGB->BGR\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def keep_top_left_percent(image, k: float):\n",
    "    \"\"\"\n",
    "    Keep only the top and left k-percent of pixels (an upside-down 'L'), black out the rest.\n",
    "    Works on either\n",
    "      • numpy.ndarray (H×W×C BGR)  or\n",
    "      • torch.Tensor   (C×H×W RGB floats [0,1] or [0,255], on CPU or CUDA)\n",
    "    \"\"\"\n",
    "    # --- validate k ---\n",
    "    if not (0 < k <= 1):\n",
    "        raise ValueError(\"Parameter k must be a float between 0 and 1.\")\n",
    "\n",
    "    # --- branch on type ---\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        # print(\"Is tensor\")\n",
    "        # tensor path: C×H×W\n",
    "        C, H, W = image.shape\n",
    "        top_h = math.ceil(k * H)\n",
    "        left_w = math.ceil(k * W)\n",
    "\n",
    "        # build mask on same device & dtype\n",
    "        mask2d = torch.zeros((H, W), dtype=image.dtype, device=image.device)\n",
    "        mask2d[:top_h, :] = 1\n",
    "        mask2d[:, :left_w] = 1\n",
    "\n",
    "        # expand to C×H×W\n",
    "        mask3d = mask2d.unsqueeze(0).expand(C, H, W)\n",
    "\n",
    "        # apply mask\n",
    "        out_t = image * mask3d\n",
    "\n",
    "        return out_t\n",
    "\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        print(\"Is not tensor!\")\n",
    "        # numpy path: H×W×C BGR\n",
    "        H, W = image.shape[:2]\n",
    "        top_h = math.ceil(k * H)\n",
    "        left_w = math.ceil(k * W)\n",
    "\n",
    "        mask = np.zeros((H, W), dtype=bool)\n",
    "        mask[:top_h, :] = True\n",
    "        mask[:, :left_w] = True\n",
    "\n",
    "        black = np.zeros_like(image)\n",
    "        out_np = np.where(mask[..., None], image, black)\n",
    "\n",
    "        return out_np\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            f\"Unsupported image type {type(image)} – must be numpy.ndarray or torch.Tensor.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def resize_torch(image0, new_size):\n",
    "  # image0 is a C×H×W float tensor on CUDA\n",
    "  # 1) send to CPU and turn into a H×W×C NumPy array\n",
    "  np_img = image0.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "  # 2) do your OpenCV resize\n",
    "  h, w = np_img.shape[:2]\n",
    "  new_w = (w * int(new_size * 10)) // 10\n",
    "  new_h = (h * int(new_size * 10)) // 10\n",
    "  resized_np = cv2.resize(np_img, (new_w, new_h))\n",
    "\n",
    "  # 3) (optional) turn back into a tensor, same dtype/device as before\n",
    "  return torch.from_numpy(resized_np).permute(2, 0, 1).to(image0.dtype).cuda()\n",
    "\n",
    "\n",
    "def crop_tensor_image(image: torch.Tensor, h1: int, w1: int, h2: int, w2: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Crop an image tensor based on bounding box coordinates.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Image tensor of shape (C, H, W), values in [0,1] or [0,255].\n",
    "        h1 (int): Top coordinate (inclusive).\n",
    "        w1 (int): Left coordinate (inclusive).\n",
    "        h2 (int): Bottom coordinate (exclusive).\n",
    "        w2 (int): Right coordinate (exclusive).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Cropped image tensor of shape (C, h2-h1, w2-w1).\n",
    "    \"\"\"\n",
    "    if image.ndim != 3:\n",
    "        raise ValueError(\"Input image must be a 3D tensor (C, H, W)\")\n",
    "\n",
    "    C, H, W = image.shape\n",
    "\n",
    "    # Clamp coordinates to image boundaries\n",
    "    h1 = max(0, min(H, h1))\n",
    "    h2 = max(0, min(H, h2))\n",
    "    w1 = max(0, min(W, w1))\n",
    "    w2 = max(0, min(W, w2))\n",
    "\n",
    "    if h1 >= h2 or w1 >= w2:\n",
    "        raise ValueError(\"Invalid bounding box: (h1, w1) must be above and to the left of (h2, w2)\")\n",
    "\n",
    "    return image[:, h1:h2, w1:w2]\n",
    "\n",
    "def poz(matrix_size, i):\n",
    "  init_i = i\n",
    "\n",
    "  if i < 1:\n",
    "    return None\n",
    "  if i > (matrix_size * matrix_size) - 1:\n",
    "    return None\n",
    "\n",
    "  max_line = matrix_size - 1\n",
    "\n",
    "  diag_nr = 1\n",
    "  x = diag_nr\n",
    "  y = 0\n",
    "  prev_poz = 0, 0\n",
    "  while True:\n",
    "    i -= 1\n",
    "    if i == 0:\n",
    "      if init_i < ((matrix_size * (matrix_size + 1)) // 2):\n",
    "        prev_x, prev_y = prev_poz\n",
    "        if prev_x == 0:\n",
    "          return x, y, prev_x + prev_y + 1, prev_x + prev_y + 1\n",
    "        else:\n",
    "          return x, y, prev_x + prev_y + 1, prev_x + prev_y\n",
    "      return x, y, matrix_size, matrix_size\n",
    "\n",
    "    if i == 1:\n",
    "      prev_poz = x, y\n",
    "\n",
    "    if x == 0 or y == max_line:\n",
    "      diag_nr += 1\n",
    "\n",
    "      if diag_nr > max_line:\n",
    "        x = max_line\n",
    "        y = diag_nr - max_line\n",
    "      else:\n",
    "        x = diag_nr\n",
    "        y = 0\n",
    "    else:\n",
    "      x -= 1\n",
    "      y += 1\n",
    "\n",
    "def poz_list(path, paths):\n",
    "  max_0, max_1 = 0, 0\n",
    "  for path1 in paths:\n",
    "    if path == path1:\n",
    "      break\n",
    "    path1 = convert_image_number_base(path1, 10, 5)\n",
    "    x, y = int(path1.split(\".jpg\")[0][2:][0]), int(path1.split(\".jpg\")[0][2:][1])\n",
    "    max_0 = max(max_0, x)\n",
    "    max_1 = max(max_1, y)\n",
    "  return int(path.split(\".jpg\")[0][2:][0]), int(path.split(\".jpg\")[0][2:][1]), max_0 + 1, max_1 + 1\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "def save_image(tensor: torch.Tensor, filename: str = \"image.png\"):\n",
    "    \"\"\"\n",
    "    Save a tensor as an image file.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Image tensor of shape (C, H, W).\n",
    "        filename (str): Path to save the image.\n",
    "    \"\"\"\n",
    "    # If the tensor is [0, 255] floats, normalize it to [0, 1]\n",
    "    if tensor.max() > 1.0:\n",
    "        tensor = tensor / 255.0\n",
    "\n",
    "    vutils.save_image(tensor, filename)\n",
    "\n",
    "def detect_black_border_depths(image_pil, black_threshold=10):\n",
    "    \"\"\"\n",
    "    Detects how much black padding exists on each edge of the image separately.\n",
    "\n",
    "    Args:\n",
    "        image_pil: PIL.Image object (RGB or grayscale).\n",
    "        black_threshold: Maximum pixel value considered \"black\" (0-255).\n",
    "\n",
    "    Returns:\n",
    "        (top, bottom, left, right): number of black pixels to crop from each side.\n",
    "    \"\"\"\n",
    "    image_np = np.array(image_pil)\n",
    "\n",
    "    if image_np.ndim == 3:\n",
    "        # Convert to grayscale if RGB\n",
    "        image_np = np.mean(image_np, axis=2)\n",
    "\n",
    "    h, w = image_np.shape\n",
    "\n",
    "    # Initialize cropping values\n",
    "    top = 0\n",
    "    bottom = 0\n",
    "    left = 0\n",
    "    right = 0\n",
    "\n",
    "    # Detect top\n",
    "    for y in range(h):\n",
    "        if np.all(image_np[y, :] <= black_threshold):\n",
    "            top += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Detect bottom\n",
    "    for y in range(h-1, -1, -1):\n",
    "        if np.all(image_np[y, :] <= black_threshold):\n",
    "            bottom += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Detect left\n",
    "    for x in range(w):\n",
    "        if np.all(image_np[:, x] <= black_threshold):\n",
    "            left += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Detect right\n",
    "    for x in range(w-1, -1, -1):\n",
    "        if np.all(image_np[:, x] <= black_threshold):\n",
    "            right += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return top, bottom, left, right\n",
    "\n",
    "\"\"\"\n",
    "def crop_black_borders(image_pil, black_threshold=10):\n",
    "    top, bottom, left, right = detect_black_border_depths(image_pil, black_threshold)\n",
    "    w, h = image_pil.size\n",
    "\n",
    "    # Calculate new box\n",
    "    left_crop = left\n",
    "    upper_crop = top\n",
    "    right_crop = w - right\n",
    "    lower_crop = h - bottom\n",
    "\n",
    "    if left_crop >= right_crop or upper_crop >= lower_crop:\n",
    "        # If everything is cropped away, return original (or could raise an error)\n",
    "        print(\"Warning: Cropping would remove entire image. Returning original.\")\n",
    "        return image_pil\n",
    "\n",
    "    return image_pil.crop((left_crop, upper_crop, right_crop, lower_crop))\n",
    "\"\"\"\n",
    "\n",
    "def torch_image_shape(image):\n",
    "  np_img = image.cpu().permute(1, 2, 0).numpy()\n",
    "  h, w = np_img.shape[:2]\n",
    "  return h, w\n",
    "\n",
    "def LightGlue_matrix_scan_diagonal_matching(\n",
    "    img_path1,\n",
    "    img_path2,\n",
    "    basepath_img2,\n",
    "    path_list,\n",
    "    i_value,\n",
    "    matrix_size,\n",
    "    output_dir,\n",
    "    ransac_thresholding=100,\n",
    "    percentage_of_image_used=1.0,\n",
    "    pair_extractor=\"SIFT\",\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "  # print(convert_image_number_base(basepath_img2, 10, 5))\n",
    "  poz_h, poz_w, nr_h, nr_w = poz_list(basepath_img2, path_list)\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"We want to crop the matrix crop {(poz_h, poz_w)} out of a {matrix_size - 1, matrix_size - 1} matrix!\")\n",
    "    print(f\"{nr_h}, {nr_w}\")\n",
    "\n",
    "  matches_maxx = []\n",
    "\n",
    "  # for iii in range(0, nr_h * 10 - 5, 5):\n",
    "  #   for jjj in range(0, nr_w * 10 - 5, 5):\n",
    "  #     i = iii / 10\n",
    "  #     j = jjj / 10\n",
    "  for i in range(nr_h):\n",
    "    for j in range(nr_w):\n",
    "      torch.cuda.empty_cache()\n",
    "      left_gray, left_origin, left_rgb = read_image(img_path1, 0.8)\n",
    "      right_gray, right_origin, right_rgb = read_image(img_path2, 0.8)\n",
    "\n",
    "      # right_height, right_width = img_path2.shape[:2]\n",
    "      # new_height = int(left_height * percentage_of_image_used)\n",
    "      # new_width = int(left_width * percentage_of_image_used)\n",
    "\n",
    "      image0 = load_image(img_path1).cuda()\n",
    "      image1 = load_image(img_path2).cuda()\n",
    "\n",
    "      image0 = resize_torch(image0, 0.8)\n",
    "      image1 = resize_torch(image1, 0.8)\n",
    "\n",
    "      # image0 = keep_top_left_percent(image0, percentage_of_image_used)\n",
    "      image1 = keep_top_left_percent(image1, percentage_of_image_used)\n",
    "\n",
    "      h, w = torch_image_shape(image0)\n",
    "\n",
    "      if verbose:\n",
    "        print(f\"Image size of {h, w}\")\n",
    "\n",
    "      h_size, w_size = h // nr_h, w // nr_w\n",
    "\n",
    "      if verbose:\n",
    "        print(f\"Tile size of {(h_size, w_size)}\")\n",
    "\n",
    "      flag = True\n",
    "\n",
    "      if verbose:\n",
    "        print(f\"h_size = {h_size}\")\n",
    "        print(f\"w_size = {w_size}\")\n",
    "\n",
    "        print(f\"i = {i}\")\n",
    "        print(f\"j = {j}\")\n",
    "\n",
    "      top_left_h = int(max(0, (j * h_size) - (h_size * 0.65)))\n",
    "      top_left_w = int(max(0, (i * w_size) - (w_size * 0.65)))\n",
    "\n",
    "      bottom_right_h = int(min(h, ((j + 1) * h_size) + (0.65 * h_size)))\n",
    "      bottom_right_w = int(min(w, ((i + 1) * w_size) + (0.65 * w_size)))\n",
    "\n",
    "      if verbose:\n",
    "        print(f\"Box proposed is {(top_left_h, top_left_w, bottom_right_h, bottom_right_w)}\")\n",
    "\n",
    "      image0 = crop_tensor_image(image0, top_left_h, top_left_w, bottom_right_h, bottom_right_w)\n",
    "\n",
    "      # cropped = crop_tensor_image(image0, top_left_h, top_left_w, bottom_right_h, bottom_right_w)\n",
    "      # save_image(image0, f\"00000_{i_value}_{i}_{j}.png\")\n",
    "      # save_image(image1, f\"00001_{i_value}.png\")\n",
    "\n",
    "      if pair_extractor == \"SIFT\":\n",
    "        extractor = SIFT(max_num_keypoints=None).eval().cuda()\n",
    "        matcher = LightGlue(features='sift', depth_confidence=-1, width_confidence=-1).cuda()\n",
    "      elif pair_extractor ==\"SuperPoint\":\n",
    "        extractor = SuperPoint(max_num_keypoints=None).cuda()\n",
    "        matcher = LightGlue(features='superpoint', depth_confidence=-1, width_confidence=-1).cuda()\n",
    "      else:\n",
    "        raise ValueError(f\"Invalid pair extractor: {pair_extractor}\")\n",
    "\n",
    "      try:\n",
    "        feats0 = extractor.extract(image0)\n",
    "        feats1 = extractor.extract(image1)\n",
    "\n",
    "        matches01 = matcher({'image0': feats0, 'image1': feats1})\n",
    "        feats0, feats1, matches01 = [rbd(x) for x in [feats0, feats1, matches01]]\n",
    "        matches = matches01['matches']\n",
    "        points0 = feats0['keypoints'][matches[..., 0]]\n",
    "        points1 = feats1['keypoints'][matches[..., 1]]\n",
    "\n",
    "        matches = np.concatenate((points0.cpu(), points1.cpu()), axis=1)\n",
    "      except Exception as e:\n",
    "        print(f\"While doing Deep Learning matching got exception: {e}\")\n",
    "        matches = []\n",
    "\n",
    "\n",
    "      if verbose:\n",
    "        print(f\"\\n\\n\\nAvem {len(matches)} matches!\\n\\n\\n\")\n",
    "\n",
    "      for ii in range(len(matches)):\n",
    "        matches[ii][0] += top_left_w\n",
    "        matches[ii][1] += top_left_h\n",
    "\n",
    "      if len(matches) > len(matches_maxx):\n",
    "        if verbose:\n",
    "          print(\"Updated maxx matches!\")\n",
    "        matches_maxx = matches\n",
    "\n",
    "  inliers, H = ransac(matches_maxx, 100, 2000)\n",
    "\n",
    "  kaze_stitched = stitch_img(left_rgb, right_rgb, H)\n",
    "\n",
    "  kaze_stitched_uint8 = (kaze_stitched * 255).astype(np.uint8)\n",
    "  image_pil = Image.fromarray(kaze_stitched_uint8)\n",
    "\n",
    "  output_filename = f\"0000-{os.path.splitext(os.path.basename(img_path2))[0]}_{pair_extractor}_{percentage_of_image_used}.jpg\"\n",
    "  output_filepath = os.path.join(output_dir, output_filename)\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"Saving at {output_filepath}\")\n",
    "\n",
    "  cropped_image = crop_black_borders(image_pil, black_threshold=10)\n",
    "\n",
    "  cropped_image.save(output_filepath, \"JPEG\", quality=95)\n",
    "\n",
    "  return output_filename\n",
    "\n",
    "def convert_image_number_base(filepath: str, b1: int, b2: int) -> str:\n",
    "    if not (1 <= b1 <= 10 and 1 <= b2 <= 10):\n",
    "        raise ValueError(\"Bases b1 and b2 must be between 1 and 10 (inclusive)\")\n",
    "\n",
    "    # Split into directory, filename, extension\n",
    "    dirname, filename = os.path.split(filepath)\n",
    "    name, ext = os.path.splitext(filename)\n",
    "\n",
    "    if ext.lower() not in ['.jpg', '.png']:\n",
    "        raise ValueError(\"File must be a .jpg or .png\")\n",
    "\n",
    "    # Ensure the number part is 4 digits\n",
    "    if not (len(name) == 4 and name.isdigit()):\n",
    "        raise ValueError(\"Filename must contain a 4-digit number\")\n",
    "\n",
    "    # Convert number from base b1 to int\n",
    "    try:\n",
    "        number = int(name, base=b1)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"The number '{name}' is not valid in base {b1}\")\n",
    "\n",
    "    # Convert from int to base b2\n",
    "    def to_base(n: int, base: int) -> str:\n",
    "        if n == 0:\n",
    "            return '0'\n",
    "        digits = []\n",
    "        while n:\n",
    "            digits.append(str(n % base))\n",
    "            n //= base\n",
    "        return ''.join(reversed(digits))\n",
    "\n",
    "    new_number = to_base(number, b2).zfill(4)\n",
    "    new_filename = new_number + ext\n",
    "    return os.path.join(dirname, new_filename) if dirname else new_filename\n",
    "\n",
    "def diagonal_submatrix(k, x, y, image_paths):\n",
    "    matrix_size = k  # Assumes a 10x10 matrix\n",
    "    \"\"\"\n",
    "    Return the k×k submatrix of a width×width grid stored in `image_paths` (row-major order),\n",
    "    starting at top-left coordinate (x, y), traversed in diagonal order.\n",
    "\n",
    "    Parameters:\n",
    "        image_paths (List[str]): Flat list of image image_paths of length width×width.\n",
    "        k (int): Size of the submatrix (k rows, k columns).\n",
    "        x (int): Row index of the submatrix's top-left corner (0-based).\n",
    "        y (int): Column index of the submatrix's top-left corner (0-based).\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The k×k submatrix's elements in diagonal order.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # There are 2k-1 diagonals, indexed by sum d = i+j from 0 to 2(k-1).\n",
    "    for d in range(2 * k - 1):\n",
    "        # For each diagonal, i goes from min(d, k-1) down to max(0, d-(k-1)).\n",
    "        start = min(d, k - 1)\n",
    "        end = max(0, d - (k - 1))\n",
    "        for i in range(start, end - 1, -1):\n",
    "            j = d - i\n",
    "            global_i = x + i\n",
    "            global_j = y + j\n",
    "            idx = global_i * matrix_size + global_j\n",
    "            result.append(image_paths[idx])\n",
    "    return result\n",
    "\n",
    "def filter_paths(paths):\n",
    "  def is_ok(x):\n",
    "    return int(x[2]) < 5 and int(x[3]) < 5\n",
    "  return [x for x in paths if is_ok(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WF-72v5pmh0y"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def crop_black_borders(image, threshold=10):\n",
    "    # Create a mask of where the image is not black\n",
    "    # (using the maximum channel value per pixel)\n",
    "    if len(image.shape) == 3:\n",
    "        mask = image.max(axis=2) > threshold\n",
    "    else:\n",
    "        mask = image > threshold\n",
    "\n",
    "    # Get coordinates of non-black pixels\n",
    "    coords = np.argwhere(mask)\n",
    "    if coords.size == 0:\n",
    "        # No non-black pixels; return original image\n",
    "        return image\n",
    "\n",
    "    # Find the bounding box of the non-black region\n",
    "    y0, x0 = coords.min(axis=0)\n",
    "    y1, x1 = coords.max(axis=0) + 1  # slices are exclusive at the top\n",
    "    cropped = image[y0:y1, x0:x1]\n",
    "    return cropped\n",
    "\"\"\"\n",
    "\n",
    "def compute_stitched_ssim(image_path, k, seam_width=10, black_thresh=10):\n",
    "    \"\"\"\n",
    "    Computes the average SSIM for seam areas between adjacent sub-images\n",
    "    in a K x K stitched image stored in 'image_path'.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): The file path to the stitched image.\n",
    "        k (int): The number of rows/columns in the stitched grid (total sub-images = k*k).\n",
    "        seam_width (int): The width (in pixels) of the region along each seam to compare.\n",
    "        black_thresh (int): Pixel intensity threshold to consider as black when cropping.\n",
    "\n",
    "    Returns:\n",
    "        average_ssim (float): The average SSIM computed over all seams.\n",
    "        seam_ssim_scores (dict): A dictionary with keys 'vertical' and 'horizontal' containing lists of SSIM scores.\n",
    "    \"\"\"\n",
    "    # Load the image using OpenCV (reads as BGR)\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    if image_bgr is None:\n",
    "        raise FileNotFoundError(f\"Could not load image from '{image_path}'\")\n",
    "\n",
    "    # Convert to RGB for consistency\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Crop out surrounding black borders\n",
    "    cropped_image = crop_black_borders(image_rgb, threshold=black_thresh)\n",
    "\n",
    "    # Get dimensions of the cropped image\n",
    "    h, w = cropped_image.shape[:2]\n",
    "\n",
    "    # Determine size of each sub-image (using integer division)\n",
    "    sub_h = h // k\n",
    "    sub_w = w // k\n",
    "\n",
    "    # Create a list to hold the sub-images\n",
    "    sub_images = []\n",
    "    for i in range(k):\n",
    "        row = []\n",
    "        for j in range(k):\n",
    "            # Extract the (i, j)th sub-image\n",
    "            x_start = j * sub_w\n",
    "            x_end = (j + 1) * sub_w\n",
    "            y_start = i * sub_h\n",
    "            y_end = (i + 1) * sub_h\n",
    "            sub_img = cropped_image[y_start:y_end, x_start:x_end]\n",
    "            row.append(sub_img)\n",
    "        sub_images.append(row)\n",
    "\n",
    "    # Prepare lists to hold SSIM scores from vertical and horizontal seams\n",
    "    vertical_scores = []\n",
    "    horizontal_scores = []\n",
    "\n",
    "    # Compute SSIM on vertical seams (between left & right adjacent sub-images)\n",
    "    for i in range(k):\n",
    "        for j in range(k - 1):\n",
    "            left_img = sub_images[i][j]\n",
    "            right_img = sub_images[i][j + 1]\n",
    "            # Select the right seam of left_img and left seam of right_img\n",
    "            left_seam = left_img[:, -seam_width:]\n",
    "            right_seam = right_img[:, :seam_width]\n",
    "            # Convert to grayscale for SSIM calculation\n",
    "            left_gray = cv2.cvtColor(left_seam, cv2.COLOR_RGB2GRAY)\n",
    "            right_gray = cv2.cvtColor(right_seam, cv2.COLOR_RGB2GRAY)\n",
    "            score, _ = ssim(left_gray, right_gray, full=True)\n",
    "            vertical_scores.append(score)\n",
    "\n",
    "    # Compute SSIM on horizontal seams (between top & bottom adjacent sub-images)\n",
    "    for i in range(k - 1):\n",
    "        for j in range(k):\n",
    "            top_img = sub_images[i][j]\n",
    "            bottom_img = sub_images[i + 1][j]\n",
    "            # Select the bottom seam of top_img and top seam of bottom_img\n",
    "            top_seam = top_img[-seam_width:, :]\n",
    "            bottom_seam = bottom_img[:seam_width, :]\n",
    "            top_gray = cv2.cvtColor(top_seam, cv2.COLOR_RGB2GRAY)\n",
    "            bottom_gray = cv2.cvtColor(bottom_seam, cv2.COLOR_RGB2GRAY)\n",
    "            score, _ = ssim(top_gray, bottom_gray, full=True)\n",
    "            horizontal_scores.append(score)\n",
    "\n",
    "    # Combine all scores and compute an average SSIM\n",
    "    all_scores = vertical_scores + horizontal_scores\n",
    "    # average_ssim = np.mean(all_scores) if all_scores else None\n",
    "    penalized_average_ssim = 1 - np.mean((np.array(all_scores) - 0.6)**2)\n",
    "\n",
    "    seam_ssim_scores = {\n",
    "        \"vertical\": vertical_scores,\n",
    "        \"horizontal\": horizontal_scores\n",
    "    }\n",
    "\n",
    "    return penalized_average_ssim, seam_ssim_scores\n",
    "\n",
    "\"\"\"\n",
    "def crop_black_borders(image, threshold=10):\n",
    "    if image.ndim == 3:\n",
    "        mask = image.max(axis=2) > threshold\n",
    "    else:\n",
    "        mask = image > threshold\n",
    "    coords = np.argwhere(mask)\n",
    "    if coords.size == 0:\n",
    "        return image\n",
    "    y0, x0 = coords.min(axis=0)\n",
    "    y1, x1 = coords.max(axis=0) + 1\n",
    "    return image[y0:y1, x0:x1]\n",
    "\"\"\"\n",
    "\n",
    "def compute_improved_scan_ssim(\n",
    "    image_path: str,\n",
    "    patch_h: int,\n",
    "    patch_w: int,\n",
    "    stride: int = 1,\n",
    "    seam_width: int = 10,\n",
    "    black_thresh: int = 10,\n",
    "    α: float = 0.5,\n",
    "    β: float = 0.4,\n",
    "    γ: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Sliding‐window scan that computes a combined seam‐quality score:\n",
    "      Q = α·(intensity SSIM)\n",
    "        + β·(edge SSIM)\n",
    "        - γ·(normalized gradient‐difference)\n",
    "    Returns:\n",
    "      vert_map, hori_map: arrays of Q‐scores\n",
    "      mean_Q, rms_Q: overall statistics\n",
    "    \"\"\"\n",
    "    # 1) load & crop\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    if img_bgr is None:\n",
    "        raise FileNotFoundError(f\"Cannot open {image_path!r}\")\n",
    "    img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    # img = crop_black_borders(img, black_thresh)\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    # 2) compute grid of sliding windows\n",
    "    n_rows = (H - patch_h)//stride + 1\n",
    "    n_cols = (W - patch_w)//stride + 1\n",
    "\n",
    "    vert_map = np.full((n_rows, n_cols-1), np.nan, dtype=np.float32)\n",
    "    hori_map = np.full((n_rows-1, n_cols), np.nan, dtype=np.float32)\n",
    "\n",
    "    # helper: gradient magnitude\n",
    "    def grad_mag(gray):\n",
    "        gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)\n",
    "        gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n",
    "        return cv2.magnitude(gx, gy)\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        y = i*stride\n",
    "        for j in range(n_cols):\n",
    "            x = j*stride\n",
    "            patch = img[y:y+patch_h, x:x+patch_w]\n",
    "\n",
    "            # prepare raw gray and edge maps\n",
    "            gray = cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)\n",
    "            edges = cv2.Canny(gray, 100, 200)\n",
    "            gmag = grad_mag(gray)\n",
    "\n",
    "            # vertical seam\n",
    "            if j+1 < n_cols:\n",
    "                # neighbor patch\n",
    "                nx = x + stride\n",
    "                neigh = img[y:y+patch_h, nx:nx+patch_w]\n",
    "                gray2 = cv2.cvtColor(neigh, cv2.COLOR_RGB2GRAY)\n",
    "                edges2 = cv2.Canny(gray2, 100, 200)\n",
    "                gmag2 = grad_mag(gray2)\n",
    "\n",
    "                L = gray[:, -seam_width:]\n",
    "                R = gray2[:, :seam_width]\n",
    "                E1 = edges[:, -seam_width:]\n",
    "                E2 = edges2[:, :seam_width]\n",
    "                G1 = gmag[:, -seam_width:]\n",
    "                G2 = gmag2[:, :seam_width]\n",
    "\n",
    "                s_int = ssim(L, R)\n",
    "                s_edge = ssim(E1, E2)\n",
    "                # normalized gradient diff\n",
    "                diff = np.abs(G1 - G2)\n",
    "                g_diff = np.mean(diff) / (np.max([G1.max(), G2.max(), 1e-6]))\n",
    "                vert_map[i,j] = α*s_int + β*s_edge - γ*g_diff\n",
    "\n",
    "            # horizontal seam\n",
    "            if i+1 < n_rows:\n",
    "                ny = y + stride\n",
    "                neigh = img[ny:ny+patch_h, x:x+patch_w]\n",
    "                gray2 = cv2.cvtColor(neigh, cv2.COLOR_RGB2GRAY)\n",
    "                edges2 = cv2.Canny(gray2, 100, 200)\n",
    "                gmag2 = grad_mag(gray2)\n",
    "\n",
    "                T = gray[-seam_width: , :]\n",
    "                B = gray2[:seam_width, :]\n",
    "                E1 = edges[-seam_width: , :]\n",
    "                E2 = edges2[:seam_width, :]\n",
    "                G1 = gmag[-seam_width: , :]\n",
    "                G2 = gmag2[:seam_width, :]\n",
    "\n",
    "                s_int = ssim(T, B)\n",
    "                s_edge = ssim(E1, E2)\n",
    "                diff = np.abs(G1 - G2)\n",
    "                g_diff = np.mean(diff) / (np.max([G1.max(), G2.max(), 1e-6]))\n",
    "                hori_map[i,j] = α*s_int + β*s_edge - γ*g_diff\n",
    "\n",
    "    # flatten valid scores\n",
    "    all_Q = np.concatenate([\n",
    "        vert_map[~np.isnan(vert_map)],\n",
    "        hori_map[~np.isnan(hori_map)]\n",
    "    ])\n",
    "    mean_Q = np.mean(all_Q) if all_Q.size else np.nan\n",
    "    rms_Q  = np.sqrt(np.mean(all_Q**2)) if all_Q.size else np.nan\n",
    "\n",
    "    return vert_map, hori_map, mean_Q, rms_Q\n",
    "\n",
    "def summarize_scores(all_m, combo_lambda=0.5, geom_shift=1.0, eps=1e-3):\n",
    "    \"\"\"\n",
    "    all_m : 1D array of your seam scores (can be negative)\n",
    "    Returns a dict with:\n",
    "      - mean\n",
    "      - minimum\n",
    "      - combo = λ·min + (1−λ)·mean\n",
    "      - geometric (with shift)\n",
    "      - harmonic (with clamping)\n",
    "    \"\"\"\n",
    "    N = all_m.size\n",
    "    mean_s = np.mean(all_m)\n",
    "    min_s  = np.min(all_m)\n",
    "\n",
    "    # combined min+mean\n",
    "    combo = combo_lambda*min_s + (1-combo_lambda)*mean_s\n",
    "\n",
    "    # geometric mean with shift\n",
    "    shifted = all_m + geom_shift\n",
    "    # ensure non-negative\n",
    "    if np.any(shifted <= 0):\n",
    "        print(\"Need all (score+shift)>0 for geometric mean\")\n",
    "    geom = shifted.prod()**(1.0/N) - geom_shift\n",
    "\n",
    "    # harmonic mean with clamped positives\n",
    "    clamped = np.maximum(all_m, eps)\n",
    "    harm = N / np.sum(1.0 / clamped)\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean_s,\n",
    "        \"min\": min_s,\n",
    "        \"combo\": combo,\n",
    "        \"geo\": geom,\n",
    "        \"harm\": harm\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "def crop_black_borders(image, threshold=10):\n",
    "    if image.ndim == 3:\n",
    "        mask = image.max(axis=2) > threshold\n",
    "    else:\n",
    "        mask = image > threshold\n",
    "    coords = np.argwhere(mask)\n",
    "    if coords.size == 0:\n",
    "        return image\n",
    "    y0, x0 = coords.min(axis=0)\n",
    "    y1, x1 = coords.max(axis=0) + 1\n",
    "    return image[y0:y1, x0:x1]\n",
    "\"\"\"\n",
    "\n",
    "def phase_seam_score(patch1, patch2, lam=1.0):\n",
    "    \"\"\"\n",
    "    Compute phase correlation between two same-sized gray patches.\n",
    "    Returns score = response - lam * (shift_mag / max_dim).\n",
    "    \"\"\"\n",
    "    # convert to float and apply a window to reduce edge artefacts\n",
    "    f1 = patch1.astype(np.float32)\n",
    "    f2 = patch2.astype(np.float32)\n",
    "    win = cv2.createHanningWindow(patch1.shape[::-1], cv2.CV_32F)\n",
    "    f1 *= win\n",
    "    f2 *= win\n",
    "\n",
    "    # compute phase correlation\n",
    "    (dx, dy), response = cv2.phaseCorrelate(f1, f2)\n",
    "    shift_mag = np.hypot(dx, dy)\n",
    "    max_dim = max(patch1.shape)\n",
    "    score = response - lam * (shift_mag / max_dim)\n",
    "    return score, response, (dx, dy)\n",
    "\n",
    "def compute_phasecorr_scan(\n",
    "    image_path: str,\n",
    "    patch_h: int,\n",
    "    patch_w: int,\n",
    "    stride: int = 1,\n",
    "    seam_width: int = 10,\n",
    "    black_thresh: int = 10,\n",
    "    lam: float = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Slides a patch_h×patch_w window across the image (by 'stride'),\n",
    "    and for each vertical/horizontal seam, computes a phase-correlation\n",
    "    based score.\n",
    "\n",
    "    Returns:\n",
    "      vert_map: (n_rows, n_cols-1) array of combined scores\n",
    "      horiz_map: (n_rows-1, n_cols) array of combined scores\n",
    "      mean_score: average of all valid M scores\n",
    "      rms_score: RMS of all valid M scores\n",
    "    \"\"\"\n",
    "    # 1) load & crop\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    if img_bgr is None:\n",
    "        raise FileNotFoundError(f\"Cannot open {image_path!r}\")\n",
    "    img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    img = crop_black_borders(img, threshold=black_thresh)\n",
    "    H, W = img.shape\n",
    "\n",
    "    # 2) sliding grid\n",
    "    n_rows = (H - patch_h)//stride + 1\n",
    "    n_cols = (W - patch_w)//stride + 1\n",
    "\n",
    "    vert_map  = np.full((n_rows, n_cols-1),  np.nan, dtype=np.float32)\n",
    "    horiz_map = np.full((n_rows-1, n_cols),  np.nan, dtype=np.float32)\n",
    "\n",
    "    # 3) scan and compute\n",
    "    for i in range(n_rows):\n",
    "        y = i*stride\n",
    "        for j in range(n_cols):\n",
    "            x = j*stride\n",
    "            patch = img[y:y+patch_h, x:x+patch_w]\n",
    "\n",
    "            # vertical seam: patch vs. right neighbor\n",
    "            if j+1 < n_cols:\n",
    "                nx = x + stride\n",
    "                neigh = img[y:y+patch_h, nx:nx+patch_w]\n",
    "                L = patch[:, -seam_width:]\n",
    "                R = neigh[:, :seam_width]\n",
    "                m, resp, shift = phase_seam_score(L, R, lam=lam)\n",
    "                vert_map[i, j] = m\n",
    "\n",
    "            # horizontal seam: patch vs. bottom neighbor\n",
    "            if i+1 < n_rows:\n",
    "                ny = y + stride\n",
    "                neigh = img[ny:ny+patch_h, x:x+patch_w]\n",
    "                T = patch[-seam_width:, :]\n",
    "                B = neigh[:seam_width, :]\n",
    "                m, resp, shift = phase_seam_score(T, B, lam=lam)\n",
    "                horiz_map[i, j] = m\n",
    "\n",
    "    # aggregate\n",
    "    all_m = np.concatenate([\n",
    "        vert_map[~np.isnan(vert_map)],\n",
    "        horiz_map[~np.isnan(horiz_map)]\n",
    "    ])\n",
    "\n",
    "    # Example usage within your scanning function:\n",
    "    all_m = np.concatenate([vert_map[~np.isnan(vert_map)],\n",
    "                            horiz_map[~np.isnan(horiz_map)]])\n",
    "    scores = summarize_scores(all_m,\n",
    "                              combo_lambda=0.5,\n",
    "                              geom_shift=1.0,\n",
    "                              eps=1e-3)\n",
    "\n",
    "    return vert_map, horiz_map, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Vz_oUdsymh3I"
   },
   "outputs": [],
   "source": [
    "def warp_convex_quad_to_rect(image_path, points):\n",
    "    \"\"\"\n",
    "    Warps a convex quadrilateral region in the image to a rectangle.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        points (list of tuple): 4 (x, y) tuples defining a convex quadrilateral.\n",
    "\n",
    "    Returns:\n",
    "        warped (np.ndarray): The resulting rectified rectangle image.\n",
    "    \"\"\"\n",
    "    if len(points) != 4:\n",
    "        raise ValueError(\"Exactly 4 points are required.\")\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not load image at {image_path}\")\n",
    "\n",
    "    # Convert input points to np.array\n",
    "    pts_src = np.array(points, dtype=np.float32)\n",
    "\n",
    "    # Compute bounding box size heuristically\n",
    "    def side_lengths(pts):\n",
    "        return [np.linalg.norm(pts[i] - pts[(i + 1) % 4]) for i in range(4)]\n",
    "\n",
    "    def estimate_dimensions(pts):\n",
    "        lengths = side_lengths(pts)\n",
    "        width = int((lengths[0] + lengths[2]) / 2)\n",
    "        height = int((lengths[1] + lengths[3]) / 2)\n",
    "        return width, height\n",
    "\n",
    "    width, height = estimate_dimensions(pts_src)\n",
    "\n",
    "    # Destination points for the warped rectangle\n",
    "    pts_dst = np.array([\n",
    "        [0, 0],\n",
    "        [width - 1, 0],\n",
    "        [width - 1, height - 1],\n",
    "        [0, height - 1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Compute homography and apply it\n",
    "    H, _ = cv2.findHomography(pts_src, pts_dst)\n",
    "    warped = cv2.warpPerspective(image, H, (width, height))\n",
    "\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XMg5QyMdLxow"
   },
   "outputs": [],
   "source": [
    "def extract_corners_function(image_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Extracts 4 corner points from the largest non-black region in an image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of 4 (x, y) corner tuples.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n",
    "\n",
    "    # Convert to grayscale and threshold non-black areas\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, mask = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        raise ValueError(\"No contours found in image.\")\n",
    "\n",
    "    # Find largest contour\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Approximate polygon\n",
    "    epsilon = 0.01 * cv2.arcLength(largest_contour, True)\n",
    "    approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
    "\n",
    "    if len(approx) == 4:\n",
    "        corners = [tuple(pt[0]) for pt in approx]\n",
    "    else:\n",
    "        # Use rotated bounding box to guarantee 4 corners\n",
    "        rect = cv2.minAreaRect(largest_contour)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        corners = [tuple(map(int, pt)) for pt in box]\n",
    "\n",
    "    return corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Dwib4ZnweLGm"
   },
   "outputs": [],
   "source": [
    "def full_pipeline(\n",
    "    input_folder: str,\n",
    "    output_folder: str,\n",
    "    verbose: bool = False\n",
    ") -> None:\n",
    "    # Padding the images\n",
    "    print(\"Padding the images...\")\n",
    "    os.makedirs(f\"{output_folder}/padded_images\", exist_ok=True)\n",
    "    pad_images(\n",
    "        input_folder = input_folder,\n",
    "        output_folder = f\"{output_folder}/padded_images\",\n",
    "    )\n",
    "    print(\"COMPLETE\\n\")\n",
    "\n",
    "    # Breaking down the images in patches\n",
    "    print(\"Breaking down the images in patches...\")\n",
    "    os.makedirs(f\"{output_folder}/patches\", exist_ok=True)\n",
    "    for filename in os.listdir(f\"{output_folder}/padded_images\"):\n",
    "        split_image_into_patches(f\"{output_folder}/padded_images/{filename}\", f\"{output_folder}/patches/{''.join(filename.split('.')[:-1])}_patches\")\n",
    "        rename_and_copy_images(f\"{output_folder}/patches/{''.join(filename.split('.')[:-1])}_patches\", f\"{output_folder}/patches/{''.join(filename.split('.')[:-1])}_reordered_patches\")\n",
    "\n",
    "    print(\"COMPLETE\\n\")\n",
    "\n",
    "    # Running KAZE\n",
    "    print(\"Running KAZE...\")\n",
    "\n",
    "    os.makedirs(f\"{output_folder}/KAZE\", exist_ok=True)\n",
    "    folder_paths = [path for path in os.listdir(f\"{output_folder}/patches\") if 'reordered' in path]\n",
    "\n",
    "    for folder_name in tqdm(folder_paths, desc=\"Looping Through all patch-folders\", position = 0):\n",
    "        # The folder path without the \"_reordered_patches\" part\n",
    "        os.makedirs(f\"{output_folder}/KAZE/{folder_name[:-18]}\", exist_ok=True)\n",
    "\n",
    "        input_path = f\"{output_folder}/patches/{folder_name}\"\n",
    "        output_path = f\"{output_folder}/KAZE/{folder_name[:-18]}\"\n",
    "        paths = sorted(os.listdir(input_path))\n",
    "\n",
    "        for i in tqdm(range(0, len(paths)), desc=\"Stitching all patches\", position=1, leave=False):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            elif i == 1:\n",
    "                try:\n",
    "                    stitched_img_path = KAZE_matching(\n",
    "                        img_path1=os.path.join(input_path, paths[i - 1]),\n",
    "                        img_path2=os.path.join(input_path, paths[i]),\n",
    "                        output_dir=output_path,\n",
    "                        kaze_thresholding=0.7,\n",
    "                        ransac_thresholding=100,\n",
    "                        percentage_of_image_used=1.0\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"KAZE failed after {i} stitchings.\\nReason: {str(e)}\")\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                stitched_img_path = KAZE_matching(\n",
    "                    img_path1=os.path.join(output_path, f\"0000-{paths[i - 1][:-4]}_KAZE_0.7_1.0.jpg\"),\n",
    "                    img_path2=os.path.join(input_path, paths[i]),\n",
    "                    output_dir=output_path,\n",
    "                    kaze_thresholding=0.7,\n",
    "                    ransac_thresholding=100,\n",
    "                    percentage_of_image_used=1.0\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"KAZE failed after {i} stitchings.\\nReason: {str(e)}\")\n",
    "                break\n",
    "\n",
    "\n",
    "    # Running SuperPoint\n",
    "    print(\"Running SuperPoint...\")\n",
    "\n",
    "    os.makedirs(f\"{output_folder}/SuperPoint\", exist_ok=True)\n",
    "    folder_paths = [path for path in os.listdir(f\"{output_folder}/patches\") if 'reordered' not in path]\n",
    "\n",
    "    for folder_name in tqdm(folder_paths, desc=\"Looping Through all patch-folders\", position = 0):\n",
    "        # The folder path without the \"_reordered_patches\" part\n",
    "        os.makedirs(f\"{output_folder}/SuperPoint/{folder_name[:-8]}\", exist_ok=True)\n",
    "\n",
    "        input_path = f\"{output_folder}/patches/{folder_name}\"\n",
    "        output_path = f\"{output_folder}/SuperPoint/{folder_name[:-8]}\"\n",
    "        paths = sorted(os.listdir(input_path))\n",
    "        paths = diagonal_submatrix(5, 0, 0, paths)\n",
    "\n",
    "        # print(paths)\n",
    "\n",
    "        for i in tqdm(range(0, len(paths)), desc=\"Stitching all patches\", position=1, leave=False):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            elif i == 1:\n",
    "                try:\n",
    "                    stitched_img_path = LightGlue_matrix_scan_diagonal_matching(\n",
    "                        img_path1 = os.path.join(input_path, paths[i - 1]),\n",
    "                        img_path2 = os.path.join(input_path, paths[i]),\n",
    "                        basepath_img2 = paths[i],\n",
    "                        path_list = paths,\n",
    "                        i_value = i,\n",
    "                        matrix_size = 5,\n",
    "                        output_dir=output_path,\n",
    "                        ransac_thresholding=100,\n",
    "                        percentage_of_image_used=1.0,\n",
    "                        pair_extractor=\"SuperPoint\",\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"SuperPoint failed after {i} stitchings.\\nReason: {str(e)}\")\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                stitched_img_path = LightGlue_matrix_scan_diagonal_matching(\n",
    "                    img_path1 = os.path.join(output_path, f\"0000-{paths[i - 1][:-4]}_SuperPoint_1.0.jpg\"),\n",
    "                    img_path2 = os.path.join(input_path, paths[i]),\n",
    "                    basepath_img2 = paths[i],\n",
    "                    path_list = paths,\n",
    "                    i_value = i,\n",
    "                    matrix_size = 5,\n",
    "                    output_dir=output_path,\n",
    "                    ransac_thresholding=100,\n",
    "                    percentage_of_image_used=1.0,\n",
    "                    pair_extractor=\"SuperPoint\",\n",
    "                    verbose=verbose\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"SuperPoint failed after {i} stitchings.\\nReason: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    print(\"COMPLETE\\n\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Running SIFT\n",
    "    print(\"Running SIFT...\")\n",
    "\n",
    "    os.makedirs(f\"{output_folder}/SIFT\", exist_ok=True)\n",
    "    folder_paths = [path for path in os.listdir(f\"{output_folder}/patches\") if 'reordered' not in path]\n",
    "\n",
    "    for folder_name in tqdm(folder_paths, desc=\"Looping Through all patch-folders\", position = 0):\n",
    "        # The folder path without the \"_reordered_patches\" part\n",
    "        os.makedirs(f\"{output_folder}/SIFT/{folder_name[:-8]}\", exist_ok=True)\n",
    "\n",
    "        input_path = f\"{output_folder}/patches/{folder_name}\"\n",
    "        output_path = f\"{output_folder}/SIFT/{folder_name[:-8]}\"\n",
    "        paths = sorted(os.listdir(input_path))\n",
    "        paths = diagonal_submatrix(5, 0, 0, paths)\n",
    "\n",
    "        # print(paths)\n",
    "\n",
    "        for i in tqdm(range(0, len(paths)), desc=\"Stitching all patches\", position=1, leave=False):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            elif i == 1:\n",
    "                try:\n",
    "                    stitched_img_path = LightGlue_matrix_scan_diagonal_matching(\n",
    "                        img_path1 = os.path.join(input_path, paths[i - 1]),\n",
    "                        img_path2 = os.path.join(input_path, paths[i]),\n",
    "                        basepath_img2 = paths[i],\n",
    "                        path_list = paths,\n",
    "                        i_value = i,\n",
    "                        matrix_size = 5,\n",
    "                        output_dir=output_path,\n",
    "                        ransac_thresholding=100,\n",
    "                        percentage_of_image_used=1.0,\n",
    "                        pair_extractor=\"SIFT\",\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"SIFT failed after {i} stitchings.\\nReason: {str(e)}\")\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                stitched_img_path = LightGlue_matrix_scan_diagonal_matching(\n",
    "                    img_path1 = os.path.join(output_path, f\"0000-{paths[i - 1][:-4]}_SIFT_1.0.jpg\"),\n",
    "                    img_path2 = os.path.join(input_path, paths[i]),\n",
    "                    basepath_img2 = paths[i],\n",
    "                    path_list = paths,\n",
    "                    i_value = i,\n",
    "                    matrix_size = 5,\n",
    "                    output_dir=output_path,\n",
    "                    ransac_thresholding=100,\n",
    "                    percentage_of_image_used=1.0,\n",
    "                    pair_extractor=\"SIFT\",\n",
    "                    verbose=verbose\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"SIFT failed after {i} stitchings.\\nReason: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Homography the final images\n",
    "    for folder_name in tqdm(folder_paths, desc=\"Looping Through all patch-folders\", position = 0):\n",
    "        input_path_to_kaze_image = f\"{output_folder}/KAZE/{folder_name[:-8]}/0000-0024_KAZE_0.7_1.0.jpg\"\n",
    "        input_path_to_superpoint_image = f\"{output_folder}/Superpoint/{folder_name[:-8]}/0000-0024_SuperPoint_1.0.jpg\"\n",
    "        input_path_to_sift_image = f\"{output_folder}/SIFT/{folder_name[:-8]}/0000-0024_SIFT_1.0.jpg\"\n",
    "\n",
    "        output_path_to_kaze_image = f\"{output_folder}/KAZE/{folder_name[:-8]}/homography_0000-0024_KAZE_0.7_1.0.jpg\"\n",
    "        output_path_to_superpoint_image = f\"{output_folder}/Superpoint/{folder_name[:-8]}/homography_0000-0024_SuperPoint_1.0.jpg\"\n",
    "        output_path_to_sift_image = f\"{output_folder}/SIFT/{folder_name[:-8]}/homography_0000-0024_SIFT_1.0.jpg\"\n",
    "\n",
    "\n",
    "        # points = [(1155, 442), (939, 4753), (6813, 4561), (7141, 245)]\n",
    "\n",
    "        points = extract_corners_function(input_path_to_kaze_image)\n",
    "        warped_image = warp_convex_quad_to_rect(input_path_to_kaze_image, points)\n",
    "        cv2.imwrite(output_path_to_kaze_image, warped_image)\n",
    "\n",
    "        points = extract_corners_function(input_path_to_superpoint_image)\n",
    "        warped_image = warp_convex_quad_to_rect(input_path_to_superpoint_image, points)\n",
    "        cv2.imwrite(output_path_to_superpoint_image, warped_image)\n",
    "\n",
    "        points = extract_corners_function(input_path_to_sift_image)\n",
    "        warped_image = warp_convex_quad_to_rect(input_path_to_sift_image, points)\n",
    "        cv2.imwrite(output_path_to_sift_image, warped_image)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculating and storing metrics\n",
    "\n",
    "    output_scores = []\n",
    "\n",
    "    folder_paths = [path for path in os.listdir(f\"{output_folder}/patches\") if 'reordered' not in path]\n",
    "\n",
    "    for folder_name in tqdm(folder_paths, desc=\"Looping Through all patch-folders\", position = 0):\n",
    "        dict_metrics = {\n",
    "            \"Image\": folder_name[:-8]\n",
    "        }\n",
    "\n",
    "        path_to_kaze_image = f\"{output_folder}/KAZE/{folder_name[:-8]}/homography_0000-0024_KAZE_0.7_1.0.jpg\"\n",
    "        path_to_superpoint_image = f\"{output_folder}/Superpoint/{folder_name[:-8]}/homography_0000-0024_SuperPoint_1.0.jpg\"\n",
    "        path_to_sift_image = f\"{output_folder}/SIFT/{folder_name[:-8]}/homography_0000-0024_SIFT_1.0.jpg\"\n",
    "\n",
    "        # KAZE IMAGE\n",
    "\n",
    "        if os.path.exists(path_to_kaze_image):\n",
    "            avg_ssim, _ = compute_stitched_ssim(path_to_kaze_image, 5, seam_width=50, black_thresh=10)\n",
    "            _, _, μ, _ = compute_improved_scan_ssim(\n",
    "                path_to_kaze_image,\n",
    "                patch_h=200, patch_w=200,\n",
    "                stride=50, seam_width=10,\n",
    "                black_thresh=10,\n",
    "                α=0.2, β=0.4, γ=0.4\n",
    "            )\n",
    "            _, _, scores = compute_phasecorr_scan(\n",
    "                path_to_kaze_image,\n",
    "                patch_h   = 200,\n",
    "                patch_w   = 200,\n",
    "                stride    = 50,\n",
    "                seam_width= 10,\n",
    "                black_thresh=10,\n",
    "                lam=1.0\n",
    "            )\n",
    "            phase_corr_score = scores[\"mean\"]\n",
    "\n",
    "            dict_metrics[\"KAZE\"] = {\n",
    "                \"SSIM_score\": float(avg_ssim),\n",
    "                \"MQ_score\":float(μ),\n",
    "                \"Phase_Correlation_score\": float(phase_corr_score)\n",
    "            }\n",
    "        else:\n",
    "            dict_metrics[\"KAZE\"] = {\n",
    "                \"SSIM_score\": -1,\n",
    "                \"MQ_score\": -1,\n",
    "                \"Phase_Correlation_score\": -1\n",
    "            }\n",
    "\n",
    "        # SuperPoint IMAGE\n",
    "\n",
    "        if os.path.exists(path_to_superpoint_image):\n",
    "            avg_ssim, _ = compute_stitched_ssim(path_to_superpoint_image, 5, seam_width=50, black_thresh=10)\n",
    "            _, _, μ, _ = compute_improved_scan_ssim(\n",
    "                path_to_superpoint_image,\n",
    "                patch_h=200, patch_w=200,\n",
    "                stride=50, seam_width=10,\n",
    "                black_thresh=10,\n",
    "                α=0.2, β=0.4, γ=0.4\n",
    "            )\n",
    "            _, _, scores = compute_phasecorr_scan(\n",
    "                path_to_superpoint_image,\n",
    "                patch_h   = 200,\n",
    "                patch_w   = 200,\n",
    "                stride    = 50,\n",
    "                seam_width= 10,\n",
    "                black_thresh=10,\n",
    "                lam=1.0\n",
    "            )\n",
    "            phase_corr_score = scores[\"mean\"]\n",
    "\n",
    "            dict_metrics[\"SuperPoint\"] = {\n",
    "                \"SSIM_score\": float(avg_ssim),\n",
    "                \"MQ_score\":float(μ),\n",
    "                \"Phase_Correlation_score\": float(phase_corr_score)\n",
    "            }\n",
    "        else:\n",
    "            dict_metrics[\"SuperPoint\"] = {\n",
    "                \"SSIM_score\": -1,\n",
    "                \"MQ_score\": -1,\n",
    "                \"Phase_Correlation_score\": -1\n",
    "            }\n",
    "\n",
    "        # SIFT IMAGE\n",
    "\n",
    "        if os.path.exists(path_to_sift_image):\n",
    "            print(\"Aici???\")\n",
    "            avg_ssim, _ = compute_stitched_ssim(path_to_sift_image, 5, seam_width=50, black_thresh=10)\n",
    "            _, _, μ, _ = compute_improved_scan_ssim(\n",
    "                path_to_sift_image,\n",
    "                patch_h=200, patch_w=200,\n",
    "                stride=50, seam_width=10,\n",
    "                black_thresh=10,\n",
    "                α=0.2, β=0.4, γ=0.4\n",
    "            )\n",
    "            _, _, scores = compute_phasecorr_scan(\n",
    "                path_to_sift_image,\n",
    "                patch_h   = 200,\n",
    "                patch_w   = 200,\n",
    "                stride    = 50,\n",
    "                seam_width= 10,\n",
    "                black_thresh=10,\n",
    "                lam=1.0\n",
    "            )\n",
    "            phase_corr_score = scores[\"mean\"]\n",
    "\n",
    "            dict_metrics[\"SIFT\"] = {\n",
    "                \"SSIM_score\": float(avg_ssim),\n",
    "                \"MQ_score\":float(μ),\n",
    "                \"Phase_Correlation_score\": float(phase_corr_score)\n",
    "            }\n",
    "            print(dict_metrics[\"SIFT\"])\n",
    "        else:\n",
    "            dict_metrics[\"SIFT\"] = {\n",
    "                \"SSIM_score\": -1,\n",
    "                \"MQ_score\": -1,\n",
    "                \"Phase_Correlation_score\": -1\n",
    "            }\n",
    "\n",
    "        print(dict_metrics)\n",
    "\n",
    "        output_scores.append(dict_metrics)\n",
    "\n",
    "    with open(f\"{output_folder}/output.json\", \"w\", encoding=\"utf-8\") as g:\n",
    "        json.dump(output_scores, g, ensure_ascii=False, indent=2)\n",
    "    print(\"COMPLETE\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLVHmsWnee0I",
    "outputId": "a467c27c-4394-4571-a72f-63dd7643ae2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding the images...\n",
      "Padded and saved: 2.png → Dataset_saved/images_result_full/padded_images/2.png\n",
      "Padded and saved: 9.png → Dataset_saved/images_result_full/padded_images/9.png\n",
      "Padded and saved: 6.png → Dataset_saved/images_result_full/padded_images/6.png\n",
      "Padded and saved: 10.png → Dataset_saved/images_result_full/padded_images/10.png\n",
      "Padded and saved: 5.png → Dataset_saved/images_result_full/padded_images/5.png\n",
      "Padded and saved: 8.png → Dataset_saved/images_result_full/padded_images/8.png\n",
      "Padded and saved: 4.png → Dataset_saved/images_result_full/padded_images/4.png\n",
      "Padded and saved: 3.png → Dataset_saved/images_result_full/padded_images/3.png\n",
      "Padded and saved: 1.png → Dataset_saved/images_result_full/padded_images/1.png\n",
      "Padded and saved: 7.png → Dataset_saved/images_result_full/padded_images/7.png\n",
      "Padded and saved: 11.png → Dataset_saved/images_result_full/padded_images/11.png\n",
      "Padded and saved: 12.png → Dataset_saved/images_result_full/padded_images/12.png\n",
      "COMPLETE\n",
      "\n",
      "Breaking down the images in patches...\n",
      "COMPLETE\n",
      "\n",
      "Running KAZE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Looping Through all patch-folders:   0%|                 | 0/12 [00:00<?, ?it/s]\n",
      "Stitching all patches:   0%|                             | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Stitching all patches:   8%|█▋                   | 2/25 [00:06<01:11,  3.11s/it]\u001b[A\n",
      "Stitching all patches:  12%|██▌                  | 3/25 [00:09<01:06,  3.02s/it]\u001b[A\n",
      "Stitching all patches:  16%|███▎                 | 4/25 [00:14<01:18,  3.73s/it]\u001b[A\n",
      "Stitching all patches:  20%|████▏                | 5/25 [00:27<02:24,  7.25s/it]\u001b[A\n",
      "Stitching all patches:  24%|█████                | 6/25 [00:34<02:15,  7.13s/it]\u001b[A\n",
      "Stitching all patches:  28%|█████▉               | 7/25 [00:46<02:32,  8.49s/it]\u001b[A\n",
      "Stitching all patches:  32%|██████▋              | 8/25 [01:06<03:25, 12.11s/it]\u001b[A\n",
      "Stitching all patches:  36%|███████▌             | 9/25 [01:19<03:21, 12.61s/it]\u001b[A\n",
      "Stitching all patches:  40%|████████            | 10/25 [01:31<03:04, 12.28s/it]\u001b[A\n",
      "Stitching all patches:  44%|████████▊           | 11/25 [02:22<05:36, 24.04s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "full_pipeline(\n",
    "    input_folder = INPUT_FOLDER,\n",
    "    output_folder = OUTPUT_FOLDER,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9yEFivSlSp-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
