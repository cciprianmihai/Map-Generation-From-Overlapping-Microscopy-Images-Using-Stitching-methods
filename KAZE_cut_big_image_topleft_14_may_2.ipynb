{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Y-k3gLY-dCQ",
    "outputId": "af4b0b6c-7183-48cf-a2ff-a045b4a2cccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oH1PEMvaIjCe",
    "outputId": "6356d04a-6df6-48aa-ed86-06606edf1dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7pG8AqflWXgp"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "plt.rcParams['figure.figsize'] = [15, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nBMYwmvpXV_2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read image and convert them to gray!!\n",
    "def read_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found at path: {path}\")\n",
    "\n",
    "    # Resize to 80% of original size\n",
    "    height, width = img.shape[:2]\n",
    "    new_size = (int(width * 0.8), int(height * 0.8))\n",
    "    img = cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return img_gray, img, img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gIi3kAxQXWCi"
   },
   "outputs": [],
   "source": [
    "def KAZE(img):\n",
    "    kazeDetector = cv2.KAZE_create()  # Create KAZE detector (supports both AKAZE and KAZE)\n",
    "\n",
    "    kp, des = kazeDetector.detectAndCompute(img, None)\n",
    "    return kp, des\n",
    "\n",
    "def plot_kaze(gray, rgb, kp):\n",
    "    tmp = rgb.copy()\n",
    "    img = cv2.drawKeypoints(gray, kp, tmp, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PruQr9skXWEu"
   },
   "outputs": [],
   "source": [
    "def matcher(kp1, des1, img1, kp2, des2, img2, threshold):\n",
    "    # BFMatcher with default params\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1,des2, k=2)\n",
    "\n",
    "    # Apply ratio test\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < threshold*n.distance:\n",
    "            good.append([m])\n",
    "\n",
    "    matches = []\n",
    "    for pair in good:\n",
    "        matches.append(list(kp1[pair[0].queryIdx].pt + kp2[pair[0].trainIdx].pt))\n",
    "\n",
    "    matches = np.array(matches)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Pg30VA37XWHm"
   },
   "outputs": [],
   "source": [
    "def homography(pairs):\n",
    "    rows = []\n",
    "    for i in range(pairs.shape[0]):\n",
    "        p1 = np.append(pairs[i][0:2], 1)\n",
    "        p2 = np.append(pairs[i][2:4], 1)\n",
    "        row1 = [0, 0, 0, p1[0], p1[1], p1[2], -p2[1]*p1[0], -p2[1]*p1[1], -p2[1]*p1[2]]\n",
    "        row2 = [p1[0], p1[1], p1[2], 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1], -p2[0]*p1[2]]\n",
    "        rows.append(row1)\n",
    "        rows.append(row2)\n",
    "    rows = np.array(rows)\n",
    "    U, s, V = np.linalg.svd(rows)\n",
    "    H = V[-1].reshape(3, 3)\n",
    "    H = H/H[2, 2] # standardize to let w*H[2,2] = 1\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i8dxyOAqYjwX"
   },
   "outputs": [],
   "source": [
    "def random_point(matches, k=4):\n",
    "    idx = random.sample(range(len(matches)), k)\n",
    "    point = [matches[i] for i in idx ]\n",
    "    return np.array(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yYGqF3S3Yjyw"
   },
   "outputs": [],
   "source": [
    "def get_error(points, H):\n",
    "    num_points = len(points)\n",
    "    all_p1 = np.concatenate((points[:, 0:2], np.ones((num_points, 1))), axis=1)\n",
    "    all_p2 = points[:, 2:4]\n",
    "    estimate_p2 = np.zeros((num_points, 2))\n",
    "    for i in range(num_points):\n",
    "        temp = np.dot(H, all_p1[i])\n",
    "        estimate_p2[i] = (temp/temp[2])[0:2] # set index 2 to 1 and slice the index 0, 1\n",
    "    # Compute error\n",
    "    errors = np.linalg.norm(all_p2 - estimate_p2 , axis=1) ** 2\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ca5B0KEZYj01"
   },
   "outputs": [],
   "source": [
    "def ransac(matches, threshold, iters):\n",
    "    num_best_inliers = 0\n",
    "\n",
    "    for i in range(iters):\n",
    "        points = random_point(matches)\n",
    "        H = homography(points)\n",
    "\n",
    "        #  avoid dividing by zero\n",
    "        if np.linalg.matrix_rank(H) < 3:\n",
    "            continue\n",
    "\n",
    "        errors = get_error(matches, H)\n",
    "        idx = np.where(errors < threshold)[0]\n",
    "        inliers = matches[idx]\n",
    "\n",
    "        num_inliers = len(inliers)\n",
    "        if num_inliers > num_best_inliers:\n",
    "            best_inliers = inliers.copy()\n",
    "            num_best_inliers = num_inliers\n",
    "            best_H = H.copy()\n",
    "\n",
    "    # print(\"inliers/matches: {}/{}\".format(num_best_inliers, len(matches)))\n",
    "    return best_inliers, best_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T4Jo0_6IYnGC"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def stitch_img(left, right, H):\n",
    "    print(\"Stitching image ...\")\n",
    "\n",
    "    # Normalize images to float in [0,1]\n",
    "    left = cv2.normalize(left.astype('float'), None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "    right = cv2.normalize(right.astype('float'), None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Get dimensions of both images\n",
    "    height_l, width_l, _ = left.shape\n",
    "    height_r, width_r, _ = right.shape\n",
    "\n",
    "    # Compute corners for the left image and transform them using H\n",
    "    corners_left = np.array([[0, 0, 1],\n",
    "                             [width_l, 0, 1],\n",
    "                             [width_l, height_l, 1],\n",
    "                             [0, height_l, 1]]).T  # shape (3,4)\n",
    "    warped_corners_left = H @ corners_left\n",
    "    # Normalize homogeneous coordinates:\n",
    "    warped_corners_left /= warped_corners_left[2, :]\n",
    "\n",
    "    # Compute corners for the right image (identity transform)\n",
    "    corners_right = np.array([[0, 0, 1],\n",
    "                              [width_r, 0, 1],\n",
    "                              [width_r, height_r, 1],\n",
    "                              [0, height_r, 1]]).T  # shape (3,4)\n",
    "\n",
    "    # Combine corners to find overall bounds\n",
    "    all_x = np.concatenate((warped_corners_left[0, :], corners_right[0, :]))\n",
    "    all_y = np.concatenate((warped_corners_left[1, :], corners_right[1, :]))\n",
    "\n",
    "    min_x, max_x = np.min(all_x), np.max(all_x)\n",
    "    min_y, max_y = np.min(all_y), np.max(all_y)\n",
    "\n",
    "    # Create a translation matrix to shift all images so that no coordinate is negative\n",
    "    tx = -min_x if min_x < 0 else 0\n",
    "    ty = -min_y if min_y < 0 else 0\n",
    "    translation_mat = np.array([[1, 0, tx],\n",
    "                                [0, 1, ty],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "    # New canvas size: use ceiling to ensure full coverage\n",
    "    width_new = int(np.ceil(max_x - min_x))\n",
    "    height_new = int(np.ceil(max_y - min_y))\n",
    "    size = (width_new, height_new)\n",
    "\n",
    "    # Warp left image with the composite transform: translation * H\n",
    "    warped_left = cv2.warpPerspective(left, translation_mat @ H, size)\n",
    "    # Warp right image with just the translation matrix (identity warp + translation)\n",
    "    warped_right = cv2.warpPerspective(right, translation_mat, size)\n",
    "\n",
    "    # Blend the two warped images.\n",
    "    # Define a black pixel threshold (all zeros)\n",
    "    black = np.zeros(3, dtype=np.float32)\n",
    "    stitch_image = np.zeros_like(warped_left)\n",
    "\n",
    "    # Iterate over every pixel (this loop might be slow for large images,\n",
    "    # and vectorized alternatives exist, but here we show the logic clearly)\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(height_new), desc=\"Blending\"):\n",
    "        for j in range(width_new):\n",
    "            pixel_left = warped_left[i, j, :]\n",
    "            pixel_right = warped_right[i, j, :]\n",
    "\n",
    "            left_valid = not np.allclose(pixel_left, black)\n",
    "            right_valid = not np.allclose(pixel_right, black)\n",
    "\n",
    "            if left_valid and not right_valid:\n",
    "                stitch_image[i, j, :] = pixel_left\n",
    "            elif right_valid and not left_valid:\n",
    "                stitch_image[i, j, :] = pixel_right\n",
    "            elif left_valid and right_valid:\n",
    "                stitch_image[i, j, :] = (pixel_left + pixel_right) / 2\n",
    "            # else: remains black\n",
    "\n",
    "    return stitch_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mxTl9rNBIaVH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mask_image_top_left(img_gray, k):\n",
    "    \"\"\"\n",
    "    Keeps the top k% rows and left-most k% columns intact, rest set to black.\n",
    "\n",
    "    Parameters:\n",
    "        img_gray (np.ndarray): Grayscale image.\n",
    "        k (float): Percentage (0 < k <= 100) of image to retain in top and left.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Modified image.\n",
    "    \"\"\"\n",
    "    if not (0 < k <= 100):\n",
    "        raise ValueError(\"k must be between 0 and 100 (exclusive of 0).\")\n",
    "\n",
    "    h, w = img_gray.shape\n",
    "    mask = np.zeros_like(img_gray)\n",
    "\n",
    "    # Calculate cutoff indices\n",
    "    cutoff_row = int(h * k / 100)\n",
    "    cutoff_col = int(w * k / 100)\n",
    "\n",
    "    # Retain top k% rows\n",
    "    mask[:cutoff_row, :] = img_gray[:cutoff_row, :]\n",
    "\n",
    "    # Retain left-most k% columns\n",
    "    mask[:, :cutoff_col] = img_gray[:, :cutoff_col]\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AHgImzC0LnN9"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def detect_black_border_depths(image_pil, black_threshold=10):\n",
    "    \"\"\"\n",
    "    Detects how much black padding exists on each edge of the image separately.\n",
    "\n",
    "    Args:\n",
    "        image_pil: PIL.Image object (RGB or grayscale).\n",
    "        black_threshold: Maximum pixel value considered \"black\" (0-255).\n",
    "\n",
    "    Returns:\n",
    "        (top, bottom, left, right): number of black pixels to crop from each side.\n",
    "    \"\"\"\n",
    "    image_np = np.array(image_pil)\n",
    "\n",
    "    if image_np.ndim == 3:\n",
    "        # Convert to grayscale if RGB\n",
    "        image_np = np.mean(image_np, axis=2)\n",
    "\n",
    "    h, w = image_np.shape\n",
    "\n",
    "    # Initialize cropping values\n",
    "    top = 0\n",
    "    bottom = 0\n",
    "    left = 0\n",
    "    right = 0\n",
    "\n",
    "    # Detect top\n",
    "    for y in range(h):\n",
    "        if np.all(image_np[y, :] <= black_threshold):\n",
    "            top += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Detect bottom\n",
    "    for y in range(h-1, -1, -1):\n",
    "        if np.all(image_np[y, :] <= black_threshold):\n",
    "            bottom += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Detect left\n",
    "    for x in range(w):\n",
    "        if np.all(image_np[:, x] <= black_threshold):\n",
    "            left += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Detect right\n",
    "    for x in range(w-1, -1, -1):\n",
    "        if np.all(image_np[:, x] <= black_threshold):\n",
    "            right += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return top, bottom, left, right\n",
    "\n",
    "def crop_black_borders(image_pil, black_threshold=10):\n",
    "    \"\"\"\n",
    "    Crop black borders independently from each edge.\n",
    "\n",
    "    Args:\n",
    "        image_pil: PIL.Image object (RGB or grayscale).\n",
    "        black_threshold: Maximum pixel value considered \"black\" (0-255).\n",
    "\n",
    "    Returns:\n",
    "        Cropped PIL.Image object.\n",
    "    \"\"\"\n",
    "    top, bottom, left, right = detect_black_border_depths(image_pil, black_threshold)\n",
    "    w, h = image_pil.size\n",
    "\n",
    "    # Calculate new box\n",
    "    left_crop = left\n",
    "    upper_crop = top\n",
    "    right_crop = w - right\n",
    "    lower_crop = h - bottom\n",
    "\n",
    "    if left_crop >= right_crop or upper_crop >= lower_crop:\n",
    "        # If everything is cropped away, return original (or could raise an error)\n",
    "        print(\"Warning: Cropping would remove entire image. Returning original.\")\n",
    "        return image_pil\n",
    "\n",
    "    return image_pil.crop((left_crop, upper_crop, right_crop, lower_crop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nx660PVOXWKD"
   },
   "outputs": [],
   "source": [
    "def KAZE_matching(img_path1, img_path2, output_dir=\"./kaze_output\", kaze_thresholding=0.7, ransac_thresholding=100, percentage_of_image_used=1.0):\n",
    "  print(img_path1)\n",
    "  left_gray, left_origin, left_rgb = read_image(img_path1)\n",
    "  print(img_path2)\n",
    "  right_gray, right_origin, right_rgb = read_image(img_path2)\n",
    "\n",
    "  right_gray = mask_image_top_left(right_gray, percentage_of_image_used * 100)\n",
    "\n",
    "  cv2.imwrite('right_gray.png', right_gray)\n",
    "\n",
    "  kp_left, des_left = KAZE(left_gray)\n",
    "  kp_right, des_right = KAZE(right_gray)\n",
    "\n",
    "  matches = matcher(kp_left, des_left, left_rgb, kp_right, des_right, right_rgb, 0.7)\n",
    "\n",
    "  inliers, H = ransac(matches, 100, 2000)\n",
    "\n",
    "  print(f\"We got {len(inliers)} final matches!\")\n",
    "\n",
    "  kaze_stitched = stitch_img(left_rgb, right_rgb, H)\n",
    "\n",
    "  kaze_stitched_uint8 = (kaze_stitched * 255).astype(np.uint8)\n",
    "  image_pil = Image.fromarray(kaze_stitched_uint8)\n",
    "\n",
    "  output_filename = f\"0000-{os.path.splitext(os.path.basename(img_path2))[0]}_KAZE_{kaze_thresholding}_{percentage_of_image_used}.jpg\"\n",
    "  output_filepath = os.path.join(output_dir, output_filename)\n",
    "\n",
    "  cropped_image = crop_black_borders(image_pil, black_threshold=10)\n",
    "  cropped_image.save(output_filepath, \"JPEG\", quality=95)\n",
    "\n",
    "  return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rjuwiG1dJA__"
   },
   "outputs": [],
   "source": [
    "def stitch_img(left, right, H):\n",
    "    print(\"Stitching image ...\")\n",
    "\n",
    "    # Normalize images to float in [0,1]\n",
    "    left = cv2.normalize(left.astype('float'), None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "    right = cv2.normalize(right.astype('float'), None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Get dimensions of both images\n",
    "    height_l, width_l, _ = left.shape\n",
    "    height_r, width_r, _ = right.shape\n",
    "\n",
    "    # Compute corners for the left image and transform them using H\n",
    "    corners_left = np.array([[0, 0, 1],\n",
    "                             [width_l, 0, 1],\n",
    "                             [width_l, height_l, 1],\n",
    "                             [0, height_l, 1]]).T  # shape (3,4)\n",
    "    warped_corners_left = H @ corners_left\n",
    "    warped_corners_left /= warped_corners_left[2, :]  # Normalize homogeneous coordinates\n",
    "\n",
    "    # Compute corners for the right image (identity transform)\n",
    "    corners_right = np.array([[0, 0, 1],\n",
    "                              [width_r, 0, 1],\n",
    "                              [width_r, height_r, 1],\n",
    "                              [0, height_r, 1]]).T  # shape (3,4)\n",
    "\n",
    "    # Combine corners to find overall bounds\n",
    "    all_x = np.concatenate((warped_corners_left[0, :], corners_right[0, :]))\n",
    "    all_y = np.concatenate((warped_corners_left[1, :], corners_right[1, :]))\n",
    "\n",
    "    min_x, max_x = np.min(all_x), np.max(all_x)\n",
    "    min_y, max_y = np.min(all_y), np.max(all_y)\n",
    "\n",
    "    # Create a translation matrix to shift all images so that no coordinate is negative\n",
    "    tx = -min_x if min_x < 0 else 0\n",
    "    ty = -min_y if min_y < 0 else 0\n",
    "    translation_mat = np.array([[1, 0, tx],\n",
    "                                [0, 1, ty],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "    # New canvas size: use ceiling to ensure full coverage\n",
    "    width_new = int(np.ceil(max_x - min_x))\n",
    "    height_new = int(np.ceil(max_y - min_y))\n",
    "    size = (width_new, height_new)\n",
    "\n",
    "    # Warp left image with the composite transform: translation_mat @ H\n",
    "    warped_left = cv2.warpPerspective(left, translation_mat @ H, size)\n",
    "    # Warp right image with just the translation matrix (identity warp + translation)\n",
    "    warped_right = cv2.warpPerspective(right, translation_mat, size)\n",
    "\n",
    "    # Vectorized blending:\n",
    "    # Create masks where any channel is non-zero (assumed as non-black)\n",
    "    mask_left = np.any(warped_left != 0, axis=2)\n",
    "    mask_right = np.any(warped_right != 0, axis=2)\n",
    "\n",
    "    # Initialize the stitched image as black\n",
    "    stitch_image = np.zeros_like(warped_left)\n",
    "\n",
    "    # Pixels only from left image\n",
    "    only_left = mask_left & ~mask_right\n",
    "    stitch_image[only_left] = warped_left[only_left]\n",
    "\n",
    "    # Pixels only from right image\n",
    "    only_right = mask_right & ~mask_left\n",
    "    stitch_image[only_right] = warped_right[only_right]\n",
    "\n",
    "    # Pixels where both images contribute (average the pixel values)\n",
    "    both = mask_left & mask_right\n",
    "    stitch_image[both] = (warped_left[both] + warped_right[both]) / 2\n",
    "\n",
    "    return stitch_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yXX9WNGvYziU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_path = \"Dataset/1_patches_diagonal/\"\n",
    "input_path = \"Dataset/1_patches/\"\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "  os.mkdir(output_path)\n",
    "\n",
    "paths = sorted(os.listdir(input_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1dtBJ6aOc90",
    "outputId": "c71af134-abf4-48a4-a9dc-012ae7f43401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000.png', '0001.png', '0002.png', '0003.png', '0004.png', '0005.png', '0006.png', '0007.png', '0008.png', '0009.png', '0010.png', '0011.png', '0012.png', '0013.png', '0014.png', '0015.png', '0016.png', '0017.png', '0018.png', '0019.png', '0020.png', '0021.png', '0022.png', '0023.png', '0024.png']\n"
     ]
    }
   ],
   "source": [
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_L4IKVAAlyAX",
    "outputId": "5795a2f2-c1ca-4e6c-ece7-2e3793d89c71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0024.png\n",
      "0000.png\n",
      "0000-0024_KAZE_0.7_1.0.jpg\n",
      "0000.png\n",
      "0001.png\n",
      "0000-0000_KAZE_0.7_1.0.jpg\n",
      "Dataset/1_patches/0000.png\n",
      "Dataset/1_patches/0001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▌                                        | 2/25 [00:04<00:50,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 636 final matches!\n",
      "Stitching image ...\n",
      "0001.png\n",
      "0002.png\n",
      "0000-0001_KAZE_0.7_1.0.jpg\n",
      "Dataset/1_patches_diagonal/0000-0001_KAZE_0.7_1.0.jpg\n",
      "Dataset/1_patches/0002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████▎                                      | 3/25 [00:05<00:39,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 61 final matches!\n",
      "Stitching image ...\n",
      "0002.png\n",
      "0003.png\n",
      "0000-0002_KAZE_0.7_1.0.jpg\n",
      "Dataset/1_patches_diagonal/0000-0002_KAZE_0.7_1.0.jpg\n",
      "Dataset/1_patches/0003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|███████                                     | 4/25 [00:06<00:32,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 31 final matches!\n",
      "Stitching image ...\n",
      "0003.png\n",
      "0004.png\n",
      "0000-0003_KAZE_0.7_1.0.jpg\n",
      "Dataset/1_patches_diagonal/0000-0003_KAZE_0.7_1.0.jpg\n",
      "Dataset/1_patches/0004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▊                                   | 5/25 [00:08<00:32,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 89 final matches!\n",
      "Stitching image ...\n",
      "0004.png\n",
      "0005.png\n",
      "0000-0004_KAZE_0.7_1.0.jpg\n",
      "Dataset/1_patches_diagonal/0000-0004_KAZE_0.7_1.0.jpg\n",
      "Dataset/1_patches/0005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▊                                   | 5/25 [00:09<00:38,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 7 final matches!\n",
      "Stitching image ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:3399: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 3 && M0.cols == 3 in function 'warpPerspective'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m     stitched_img_path \u001b[38;5;241m=\u001b[39m KAZE_matching(\n\u001b[1;32m     10\u001b[0m         img_path1 \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_path, paths[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m     11\u001b[0m         img_path2 \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_path, paths[i]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         percentage_of_image_used\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m stitched_img_path \u001b[38;5;241m=\u001b[39m \u001b[43mKAZE_matching\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_path1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0000-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_KAZE_0.7_1.0.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_path2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkaze_thresholding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mransac_thresholding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpercentage_of_image_used\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m, in \u001b[0;36mKAZE_matching\u001b[0;34m(img_path1, img_path2, output_dir, kaze_thresholding, ransac_thresholding, percentage_of_image_used)\u001b[0m\n\u001b[1;32m     16\u001b[0m inliers, H \u001b[38;5;241m=\u001b[39m ransac(matches, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inliers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m final matches!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m kaze_stitched \u001b[38;5;241m=\u001b[39m \u001b[43mstitch_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m kaze_stitched_uint8 \u001b[38;5;241m=\u001b[39m (kaze_stitched \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     23\u001b[0m image_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(kaze_stitched_uint8)\n",
      "Cell \u001b[0;32mIn[13], line 48\u001b[0m, in \u001b[0;36mstitch_img\u001b[0;34m(left, right, H)\u001b[0m\n\u001b[1;32m     46\u001b[0m warped_left \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpPerspective(left, translation_mat \u001b[38;5;241m@\u001b[39m H, size)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Warp right image with just the translation matrix (identity warp + translation)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m warped_right \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarpPerspective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslation_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Vectorized blending:\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Create masks where any channel is non-zero (assumed as non-black)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m mask_left \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39many(warped_left \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:3399: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 3 && M0.cols == 3 in function 'warpPerspective'\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
    "for i in tqdm(range(0, len(paths))):\n",
    "    print(paths[i - 1], paths[i], sep=\"\\n\")\n",
    "    print(f\"0000-{paths[i - 1][:-4]}_KAZE_0.7_1.0.jpg\")\n",
    "    if i == 0:\n",
    "        continue\n",
    "    elif i == 1:\n",
    "        stitched_img_path = KAZE_matching(\n",
    "            img_path1 = os.path.join(input_path, paths[i - 1]),\n",
    "            img_path2 = os.path.join(input_path, paths[i]),\n",
    "            output_dir=output_path,\n",
    "            kaze_thresholding=0.7,\n",
    "            ransac_thresholding=100,\n",
    "            percentage_of_image_used=1.0\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    stitched_img_path = KAZE_matching(\n",
    "        img_path1 = os.path.join(output_path, f\"0000-{paths[i - 1][:-4]}_KAZE_0.7_1.0.jpg\"),\n",
    "        img_path2 = os.path.join(input_path, paths[i]),\n",
    "        output_dir=output_path,\n",
    "        kaze_thresholding=0.7,\n",
    "        ransac_thresholding=100,\n",
    "        percentage_of_image_used=1.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
